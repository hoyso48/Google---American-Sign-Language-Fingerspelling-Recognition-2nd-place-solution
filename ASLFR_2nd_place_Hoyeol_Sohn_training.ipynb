{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c1f4380c8274391a8e5126b9d06bc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c72f475d6bc34d5d81354325dbab8b8b",
              "IPY_MODEL_a0b5e5c208f744a080d3d4168c55e1e4",
              "IPY_MODEL_68449d84fe3840489aeeab335715be73"
            ],
            "layout": "IPY_MODEL_bffbca07e97e4aab811b0047df570c1d"
          }
        },
        "c72f475d6bc34d5d81354325dbab8b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9368558ad100475391fb4e14af21da1a",
            "placeholder": "​",
            "style": "IPY_MODEL_b79316e2a4f94cffb8803371f616c355",
            "value": ""
          }
        },
        "a0b5e5c208f744a080d3d4168c55e1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ef7bd0303f44eab4bd72062e39be6e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51a6f3eb0ed54cb9a7794fafd1196cf8",
            "value": 1
          }
        },
        "68449d84fe3840489aeeab335715be73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff63ef99b0754652b56ff823958d84f5",
            "placeholder": "​",
            "style": "IPY_MODEL_05cd74b3223645dd88bcb0274652c208",
            "value": " 27/? [19:45&lt;00:00, 36.35s/it]"
          }
        },
        "bffbca07e97e4aab811b0047df570c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9368558ad100475391fb4e14af21da1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79316e2a4f94cffb8803371f616c355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50ef7bd0303f44eab4bd72062e39be6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "51a6f3eb0ed54cb9a7794fafd1196cf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff63ef99b0754652b56ff823958d84f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05cd74b3223645dd88bcb0274652c208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65428bcbcbec4928aa1631b1badd2a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94b5dbfb25744367ad25bb78098f3719",
              "IPY_MODEL_65949a79e796428d84c63a8c60393280",
              "IPY_MODEL_1fb10d27f1c940b3a54d13e04dd9fc4a"
            ],
            "layout": "IPY_MODEL_841e0d4e9dae4eb2ba9169aa6da23dbc"
          }
        },
        "94b5dbfb25744367ad25bb78098f3719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0920242efb48beb80c606e54f36355",
            "placeholder": "​",
            "style": "IPY_MODEL_f13e102e46fb403fbf244f16891fdb0b",
            "value": ""
          }
        },
        "65949a79e796428d84c63a8c60393280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3435a6413745e198b6d47d31e534ff",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd99ce4202294ca7a3ad6fc827108020",
            "value": 1
          }
        },
        "1fb10d27f1c940b3a54d13e04dd9fc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a7df609d74b4109b9c921fa57cb036f",
            "placeholder": "​",
            "style": "IPY_MODEL_c3278b176f774f50beb331b2721c7c46",
            "value": " 27/? [17:01&lt;00:00, 31.44s/it]"
          }
        },
        "841e0d4e9dae4eb2ba9169aa6da23dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0920242efb48beb80c606e54f36355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f13e102e46fb403fbf244f16891fdb0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b3435a6413745e198b6d47d31e534ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cd99ce4202294ca7a3ad6fc827108020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a7df609d74b4109b9c921fa57cb036f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3278b176f774f50beb331b2721c7c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To see how tfrecords are generated, check https://www.kaggle.com/code/hoyso48/aslfr-create-tfr\n",
        "\n",
        "NOTES\n",
        "\n",
        "1. (Colab only) You should update GCS_PATH by running code in https://www.kaggle.com/hoyso48/aslfr-get-gcs-path/edit yourself as it expires after several weeks.\n",
        "2. If you want to use GPU, set device = 'GPU' in get_strategy. You should set policy = 'float16' in CFG and dtype='float16' in get_model if you want fp16 training with GPU.\n",
        "3. It is recommended to use the following setting instead: CFG.epoch = 200, CFG.awp_lr = 0.1. It will give you a model with decent performance. set CFG.epoch = 60 and CFG.awp = False if you want the quick results. But if you want to reproduce the exact solution, see 4,5 carefully.\n",
        "4. Colab TPU runtime has recently been reduced to 3-4 hours. Therefore you cannot reproduce the solution within a single runtime(single model training in TPUv2-8 takes around ~14 hours). training should be done with the resume from the last checkpoint several times: set CFG.resume = 'auto' and run train_folds multiple times. you need to complete training of seed=42,43,44 with fold='all' to fully reproduce the solution.\n",
        "5. There is an instability in training(i.e. nan loss or huge accuracy drop occurs sometimes) with epoch=400 and awp_lr=0.2 setting. You can simply restart the training by setting CFG.resume=0. But if you don't want to throw away the training results, you can retry from the best checkpoint: set CFG.resume=best epoch(ex.130) and CFG.resume_ckpt = best ckpt(ex.'model-best.h5')"
      ],
      "metadata": {
        "id": "nc1hzQ-pEpa2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BC8ij_wcCXOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257d4c2e-bd68-4c6a-e749-11bb4b651e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/aslfr\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isdir('/content/drive/MyDrive'):\n",
        "    os.makedirs('/content/drive/MyDrive/aslfr', exist_ok=True)\n",
        "    os.chdir('/content/drive/MyDrive/aslfr')\n",
        "else:\n",
        "    os.makedirs('/content/aslfr', exist_ok=True)\n",
        "    os.chdir('/content/aslfr')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-1hEw_D_CXOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac44056-cbd1-4ee2-e978-df0b33aeeabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/612.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/612.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m501.8/612.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.5/573.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m924.6/924.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m801.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.3.5 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\n",
            "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-addons\n",
        "!pip install -q git+https://github.com/hoyso48/tf-utils@main\n",
        "!pip install -q Levenshtein\n",
        "!pip install -q keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3-8M5btQCXOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae33cc6d-2726-4a10-8f77-4fe24f9d88de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b1c9b21eea38>:15: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow Version: 2.13.0\n",
            "Python Version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import keras_nlp\n",
        "import tensorflow.keras.mixed_precision as mixed_precision\n",
        "\n",
        "from tf_utils.schedules import OneCycleLR, ListedLR\n",
        "from tf_utils.callbacks import Snapshot, SWA\n",
        "from tf_utils.learners import FGM, AWP\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from tqdm.autonotebook import tqdm\n",
        "import sklearn\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "import cv2\n",
        "import gc\n",
        "import glob\n",
        "import datetime\n",
        "\n",
        "from Levenshtein import distance\n",
        "\n",
        "print(f'Tensorflow Version: {tf.__version__}')\n",
        "print(f'Python Version: {sys.version}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9-bFAopgCXOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c8c8bf-dad3-40cc-9b33-c76d303e27de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connecting to TPU...\n",
            "REPLICAS: 8\n"
          ]
        }
      ],
      "source": [
        "# Seed all random number generators\n",
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def get_strategy(device='TPU'):\n",
        "    if \"TPU\" in device:\n",
        "        tpu = 'local' if device=='TPU-VM' else None\n",
        "        print(\"connecting to TPU...\")\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        IS_TPU = True\n",
        "\n",
        "    if device == \"GPU\"  or device==\"CPU\":\n",
        "        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "        if ngpu>1:\n",
        "            print(\"Using multi GPU\")\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "        elif ngpu==1:\n",
        "            print(\"Using single GPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "        else:\n",
        "            print(\"Using CPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "        IS_TPU = False\n",
        "\n",
        "    if device == \"GPU\":\n",
        "        print(\"Num GPUs Available: \", ngpu)\n",
        "\n",
        "    AUTO     = tf.data.experimental.AUTOTUNE\n",
        "    REPLICAS = strategy.num_replicas_in_sync\n",
        "    print(f'REPLICAS: {REPLICAS}')\n",
        "\n",
        "    return strategy, REPLICAS, IS_TPU\n",
        "\n",
        "STRATEGY, N_REPLICAS, IS_TPU = get_strategy('TPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lG8btWzoCXOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a963049-689f-492f-f06e-852ebe016768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "135\n",
            "Copying gs://kds-1dadda248a69bd8cbc18044d03c2444a9593eed795d5f632a2307052/train.csv...\n",
            "/ [1 files][  5.0 MiB/  5.0 MiB]                                                \n",
            "Operation completed over 1 objects/5.0 MiB.                                      \n",
            "Copying gs://kds-1dadda248a69bd8cbc18044d03c2444a9593eed795d5f632a2307052/character_to_prediction_index.json...\n",
            "/ [1 files][  405.0 B/  405.0 B]                                                \n",
            "Operation completed over 1 objects/405.0 B.                                      \n"
          ]
        }
      ],
      "source": [
        "#NOTE: you should run KaggleDatasets.get_gcs_path(dataset_name) in the kaggle notebook to update gcs_path as they expires after several weeks..\n",
        "#notebook: https://www.kaggle.com/hoyso48/aslfr-get-gcs-path/edit\n",
        "\n",
        "GCS_PATH = {\n",
        "            'aslfr':'gs://kds-1dadda248a69bd8cbc18044d03c2444a9593eed795d5f632a2307052',\n",
        "            'aslfr-5fold':'gs://kds-bf210dd73d66268f4c9d4897567ab8b79267f25eab4aa5c501305eef',\n",
        "            }\n",
        "\n",
        "TRAIN_FILENAMES = tf.io.gfile.glob(GCS_PATH['aslfr-5fold']+'/*.tfrecords')\n",
        "COMPETITION_PATH = GCS_PATH['aslfr']\n",
        "\n",
        "print(len(TRAIN_FILENAMES))\n",
        "!gsutil cp {COMPETITION_PATH}/train.csv .\n",
        "!gsutil cp {COMPETITION_PATH}/character_to_prediction_index.json ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7_BcvlO_CXOd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('./character_to_prediction_index.json') as json_file:\n",
        "    CHAR_TO_NUM = json.load(json_file)\n",
        "NUM_TO_CHAR = dict([(y+1,x) for x,y in CHAR_TO_NUM.items()] )\n",
        "NUM_TO_CHAR[60] = 'S'\n",
        "NUM_TO_CHAR[61] = 'E'\n",
        "NUM_TO_CHAR[0] = 'P'\n",
        "\n",
        "# LABEL_DICT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9bk4BLIOCXOd"
      },
      "outputs": [],
      "source": [
        "TABLE = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=list(NUM_TO_CHAR.values()),\n",
        "        values=list(NUM_TO_CHAR.keys()),\n",
        "    ),\n",
        "    default_value=tf.constant(-1),\n",
        "    name=\"class_weight\"\n",
        ")\n",
        "\n",
        "def preprocess_phrase(phrase, table=TABLE):\n",
        "    phrase = tf.strings.join(['S', phrase, 'E']) #'S'+ phrase + 'E'\n",
        "    phrase = tf.strings.bytes_split(phrase)\n",
        "    phrase = table.lookup(phrase)\n",
        "    return phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NnYyhUqOCXOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "7daded56-a0e2-4e5f-ab92-c400bf09b200"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                              path  file_id  sequence_id  participant_id  \\\n",
              "0  train_landmarks/5414471.parquet  5414471   1816796431             217   \n",
              "1  train_landmarks/5414471.parquet  5414471   1816825349             107   \n",
              "2  train_landmarks/5414471.parquet  5414471   1816909464               1   \n",
              "3  train_landmarks/5414471.parquet  5414471   1816967051              63   \n",
              "4  train_landmarks/5414471.parquet  5414471   1817123330              89   \n",
              "\n",
              "                      phrase  \n",
              "0               3 creekhouse  \n",
              "1            scales/kuhaylah  \n",
              "2        1383 william lanier  \n",
              "3          988 franklin lane  \n",
              "4  6920 northeast 661st road  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14dc6089-d0ec-488a-8176-e106623e52ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>file_id</th>\n",
              "      <th>sequence_id</th>\n",
              "      <th>participant_id</th>\n",
              "      <th>phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_landmarks/5414471.parquet</td>\n",
              "      <td>5414471</td>\n",
              "      <td>1816796431</td>\n",
              "      <td>217</td>\n",
              "      <td>3 creekhouse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_landmarks/5414471.parquet</td>\n",
              "      <td>5414471</td>\n",
              "      <td>1816825349</td>\n",
              "      <td>107</td>\n",
              "      <td>scales/kuhaylah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_landmarks/5414471.parquet</td>\n",
              "      <td>5414471</td>\n",
              "      <td>1816909464</td>\n",
              "      <td>1</td>\n",
              "      <td>1383 william lanier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_landmarks/5414471.parquet</td>\n",
              "      <td>5414471</td>\n",
              "      <td>1816967051</td>\n",
              "      <td>63</td>\n",
              "      <td>988 franklin lane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_landmarks/5414471.parquet</td>\n",
              "      <td>5414471</td>\n",
              "      <td>1817123330</td>\n",
              "      <td>89</td>\n",
              "      <td>6920 northeast 661st road</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14dc6089-d0ec-488a-8176-e106623e52ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-14dc6089-d0ec-488a-8176-e106623e52ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-14dc6089-d0ec-488a-8176-e106623e52ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0adec0a7-e184-403c-b55e-e6abe450ceb2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0adec0a7-e184-403c-b55e-e6abe450ceb2')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0adec0a7-e184-403c-b55e-e6abe450ceb2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67208 entries, 0 to 67207\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   path            67208 non-null  object\n",
            " 1   file_id         67208 non-null  int64 \n",
            " 2   sequence_id     67208 non-null  int64 \n",
            " 3   participant_id  67208 non-null  int64 \n",
            " 4   phrase          67208 non-null  object\n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 2.6+ MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train DataFrame\n",
        "train_df = pd.read_csv('train.csv')\n",
        "display(train_df.head())\n",
        "display(train_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1nOyjw3XCXOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bc799f-9e0c-440f-a7d5-efe166f1f1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67208 67208\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename.split('/')[-1]).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "print(count_data_items(TRAIN_FILENAMES), len(train_df))\n",
        "assert count_data_items(TRAIN_FILENAMES) == len(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SPHYR1d3CXOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad3eea5-737b-4591-8b44-dfdb6e2ed401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "543\n"
          ]
        }
      ],
      "source": [
        "#for the lip_lr function. LEFT[i] is matching with RIGHT[i](i.e LEFT[i](x) == -RIGHT[i](x)).\n",
        "#computed from https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model.obj\n",
        "\n",
        "LEFT = [\n",
        "         248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
        "         265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
        "         282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
        "         299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
        "         316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332,\n",
        "         333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
        "         350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
        "         367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n",
        "         384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400,\n",
        "         401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417,\n",
        "         418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
        "         435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
        "         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,  #LFACE\n",
        "         468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, #LHAND\n",
        "         493, 494, 495, 497, 499, 501, 503, 505, 507, 509, 511, 513, #LPOSE\n",
        "         515, 517, 519, 521, #LLEG\n",
        "         ]\n",
        "\n",
        "RIGHT = [\n",
        "         3, 7, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
        "         39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59,\n",
        "         60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n",
        "         81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102,\n",
        "         103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
        "         121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
        "         139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158,\n",
        "         159, 160, 161, 162, 163, 165, 166, 167, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179,\n",
        "         180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 198, 201,\n",
        "         202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
        "         220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
        "         238, 239, 240, 241, 242, 243, 244, 245, 246, 247, #RFACE\n",
        "        522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, #RHAND\n",
        "        490, 491, 492, 496, 498, 500, 502, 504, 506, 508, 510, 512, #RPOSE\n",
        "        514, 516, 518, 520, #RLEG\n",
        "        ]\n",
        "\n",
        "CENTRE = [\n",
        "          0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 94, 151, 152, 164, 168, 175, 195, 197, 199, 200, #FACE\n",
        "          489, #POSE\n",
        "          ]\n",
        "\n",
        "print(len(LEFT+RIGHT+CENTRE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1IVjpQZkCXOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15a06f1-150b-4441-8ccd-b2256de13967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "543\n",
            "1629\n"
          ]
        }
      ],
      "source": [
        "ROWS_PER_FRAME = 543\n",
        "MAX_LEN = 384\n",
        "CROP_LEN = MAX_LEN\n",
        "NUM_CLASSES  = len(NUM_TO_CHAR.values()) #62\n",
        "PAD = -100.\n",
        "\n",
        "LHAND = np.arange(468, 489).tolist()\n",
        "RHAND = np.arange(522, 543).tolist()\n",
        "POINT_LANDMARKS = list(range(543))\n",
        "\n",
        "NUM_NODES = len(POINT_LANDMARKS)\n",
        "CHANNELS = 3*NUM_NODES\n",
        "\n",
        "print(NUM_NODES)\n",
        "print(CHANNELS)\n",
        "\n",
        "def interp1d_(x, target_len, method='random'):\n",
        "    length = tf.shape(x)[1]\n",
        "    target_len = tf.maximum(1,target_len)\n",
        "    if method == 'random':\n",
        "        if tf.random.uniform(()) < 0.33:\n",
        "            x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bilinear')\n",
        "        else:\n",
        "            if tf.random.uniform(()) < 0.5:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'bicubic')\n",
        "            else:\n",
        "                x = tf.image.resize(x, (target_len,tf.shape(x)[1]),'nearest')\n",
        "    else:\n",
        "        x = tf.image.resize(x, (target_len,tf.shape(x)[1]),method)\n",
        "    return x\n",
        "\n",
        "def tf_nan_mean(x, axis=0, keepdims=False):\n",
        "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
        "\n",
        "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
        "    if center is None:\n",
        "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
        "    d = x - center\n",
        "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
        "\n",
        "def is_left_handed(x):\n",
        "    lhand = tf.gather(x, LHAND, axis=1)\n",
        "    rhand = tf.gather(x, RHAND, axis=1)\n",
        "    lhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(lhand), tf.int32))\n",
        "    rhand_nans = tf.reduce_sum(tf.cast(tf.math.is_nan(rhand), tf.int32))\n",
        "    return lhand_nans < rhand_nans\n",
        "\n",
        "def flip_lr(x, left=LEFT, right=RIGHT):\n",
        "    x,y,z = tf.unstack(x, axis=-1)\n",
        "    x = 1-x\n",
        "    new_x = tf.stack([x,y,z], -1)\n",
        "    new_x = tf.transpose(new_x, [1,0,2])\n",
        "    l_x = tf.gather(new_x, left, axis=0)\n",
        "    r_x = tf.gather(new_x, right, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(left)[...,None], r_x)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(right)[...,None], l_x)\n",
        "    new_x = tf.transpose(new_x, [1,0,2])\n",
        "    return new_x\n",
        "\n",
        "class Preprocess(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.point_landmarks = point_landmarks\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # if tf.rank(inputs) == 3:\n",
        "        #     x = inputs[None,...]\n",
        "        # else:\n",
        "        #     x = inputs\n",
        "        x = inputs\n",
        "        x = filter_nans_tf(x)\n",
        "        x = tf.cond(is_left_handed(x), lambda:flip_lr(x), lambda:x)\n",
        "        x = x[None,...]\n",
        "\n",
        "        if self.max_len is not None:\n",
        "            x = x[:,:self.max_len]\n",
        "        length = tf.shape(x)[1]\n",
        "\n",
        "        mean = tf_nan_mean(tf.gather(x, self.point_landmarks, axis=2), axis=[1,2], keepdims=True)\n",
        "        mean = tf.where(tf.math.is_nan(mean), tf.constant([0.5,0.5,0.],x.dtype), mean)\n",
        "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
        "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
        "\n",
        "        x = (x - mean)/std\n",
        "\n",
        "        x = tf.concat([\n",
        "            tf.reshape(x, (-1,length,3*len(self.point_landmarks))),\n",
        "            # tf.reshape(dx, (-1,length,3*len(self.point_landmarks))),\n",
        "        ], axis = -1)\n",
        "\n",
        "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QVhUtlQHCXOe"
      },
      "outputs": [],
      "source": [
        "def decode_tfrec(record_bytes):\n",
        "    features = tf.io.parse_single_example(record_bytes, {\n",
        "        'coordinates': tf.io.FixedLenFeature([], tf.string),\n",
        "        'phrase_encoded': tf.io.VarLenFeature(dtype=tf.int64),\n",
        "        'phrase': tf.io.FixedLenFeature([], tf.string),\n",
        "    })\n",
        "    out = {}\n",
        "    out['coordinates']  = tf.transpose(tf.reshape(tf.io.decode_raw(features['coordinates'], tf.float32), (-1,3,ROWS_PER_FRAME)), (0,2,1))\n",
        "    out['phrase'] = features['phrase']\n",
        "    return out\n",
        "\n",
        "def filter_nans_tf(x, ref_point=POINT_LANDMARKS):\n",
        "    mask = tf.math.logical_not(tf.reduce_all(tf.math.is_nan(tf.gather(x,ref_point,axis=1)), axis=[-2,-1]))\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "def preprocess(x, augment=False, max_len=MAX_LEN):\n",
        "    coord = x['coordinates']\n",
        "    if augment:\n",
        "        coord = augment_fn(coord, max_len=max_len)\n",
        "    coord = tf.ensure_shape(coord, (None,ROWS_PER_FRAME,3))\n",
        "\n",
        "    inp = tf.cast(Preprocess(max_len=max_len)(coord)[0],tf.float32)\n",
        "    tar = preprocess_phrase(x['phrase'])\n",
        "\n",
        "    return inp, tar\n",
        "\n",
        "def augment_phrase(phrase):\n",
        "    phrase = keras_nlp.layers.MaskedLMMaskGenerator(NUM_CLASSES-2,\n",
        "                                              mask_selection_rate=0.2,\n",
        "                                              mask_token_id=0,\n",
        "                                              mask_token_rate=0,\n",
        "                                              random_token_rate=1,\n",
        "                                              unselectable_token_ids=[0,60,61])(phrase)['token_ids']\n",
        "    return phrase\n",
        "\n",
        "def is_empty(*args):\n",
        "    return tf.shape(args[0])[0] > 1\n",
        "\n",
        "def resample(x, rate=(0.8,1.2)):\n",
        "    rate = tf.random.uniform((), rate[0], rate[1])\n",
        "    length = tf.shape(x)[0]\n",
        "    new_size = tf.cast(rate*tf.cast(length,tf.float32), tf.int32)\n",
        "    new_x = interp1d_(x, new_size)\n",
        "    return new_x\n",
        "\n",
        "def spatial_random_affine(xyz,\n",
        "    scale  = (0.8,1.2),\n",
        "    shear = (-0.15,0.15),\n",
        "    shift  = (-0.1,0.1),\n",
        "    degree = (-30,30),\n",
        "):\n",
        "    center = tf.constant([0.5,0.5])\n",
        "    if scale is not None:\n",
        "        scale = tf.random.uniform((),*scale)\n",
        "        xyz = scale*xyz\n",
        "\n",
        "    if shear is not None:\n",
        "        xy = xyz[...,:2]\n",
        "        z = xyz[...,2:]\n",
        "        shear_x = shear_y = tf.random.uniform((),*shear)\n",
        "        if tf.random.uniform(()) < 0.5:\n",
        "            shear_x = 0.\n",
        "        else:\n",
        "            shear_y = 0.\n",
        "        shear_mat = tf.identity([\n",
        "            [1.,shear_x],\n",
        "            [shear_y,1.]\n",
        "        ])\n",
        "        xy = xy @ shear_mat\n",
        "        center = center + [shear_y, shear_x]\n",
        "        xyz = tf.concat([xy,z], axis=-1)\n",
        "\n",
        "    if degree is not None:\n",
        "        xy = xyz[...,:2]\n",
        "        z = xyz[...,2:]\n",
        "        xy -= center\n",
        "        degree = tf.random.uniform((),*degree)\n",
        "        radian = degree/180*np.pi\n",
        "        c = tf.math.cos(radian)\n",
        "        s = tf.math.sin(radian)\n",
        "        rotate_mat = tf.identity([\n",
        "            [c,s],\n",
        "            [-s, c],\n",
        "        ])\n",
        "        xy = xy @ rotate_mat\n",
        "        xy = xy + center\n",
        "        xyz = tf.concat([xy,z], axis=-1)\n",
        "\n",
        "    if shift is not None:\n",
        "        shift = tf.random.uniform((),*shift)\n",
        "        xyz = xyz + shift\n",
        "\n",
        "    return xyz\n",
        "\n",
        "def temporal_crop(x, length=MAX_LEN):\n",
        "    l = tf.shape(x)[0]\n",
        "    offset = tf.random.uniform((), 0, tf.clip_by_value(l-length,1,length), dtype=tf.int32)\n",
        "    x = x[offset:offset+length]\n",
        "    return x\n",
        "\n",
        "def temporal_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n",
        "    l = tf.shape(x)[0]\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n",
        "    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l-mask_size,1,l), dtype=tf.int32)\n",
        "    x = tf.tensor_scatter_nd_update(x,tf.range(mask_offset, mask_offset+mask_size)[...,None],tf.fill([mask_size,543,3],mask_value))\n",
        "    return x\n",
        "\n",
        "def spatial_mask(x, size=(0.2,0.4), mask_value=float('nan')):\n",
        "    mask_offset_y = tf.random.uniform(())\n",
        "    mask_offset_x = tf.random.uniform(())\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_x = (mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size)\n",
        "    mask_y = (mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size)\n",
        "    mask = mask_x & mask_y\n",
        "    x = tf.where(mask[...,None], mask_value, x)\n",
        "    return x\n",
        "\n",
        "def augment_fn(x, always=False, max_len=None):\n",
        "    if tf.random.uniform(())<0.8 or always:\n",
        "        x = resample(x, (0.5,1.5))\n",
        "    # if tf.random.uniform(())<0.5 or always:\n",
        "    #     x = flip_lr(x)\n",
        "    # if max_len is not None:\n",
        "    #     x = temporal_crop(x, max_len)\n",
        "    if tf.random.uniform(())<0.75 or always:\n",
        "        x = spatial_random_affine(x)\n",
        "    # if tf.random.uniform(())<0.5 or always:\n",
        "    #     x = temporal_mask(x)\n",
        "    if tf.random.uniform(())<0.5 or always:\n",
        "        x = spatial_mask(x)\n",
        "    return x\n",
        "\n",
        "def get_tfrec_dataset(tfrecords, batch_size=64, max_len=128, target_len=64, teacher_forcing=True, drop_remainder=False, train=False, augment=False, shuffle=False, repeat=False):\n",
        "    # Initialize dataset with TFRecords\n",
        "    ds = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=tf.data.AUTOTUNE, compression_type='GZIP')\n",
        "    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n",
        "    ds = ds.map(lambda x: preprocess(x, augment=augment, max_len=max_len), tf.data.AUTOTUNE)\n",
        "\n",
        "    if train:\n",
        "        ds = ds.filter(is_empty)\n",
        "\n",
        "    if teacher_forcing:\n",
        "        ds = ds.map(lambda x,y:((x,y[:-1]),(y[1:],y[1:-1])), tf.data.AUTOTUNE)\n",
        "        if augment:\n",
        "          ds = ds.map(lambda x,y:((x[0],augment_phrase(x[1])),y), tf.data.AUTOTUNE)\n",
        "\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle)\n",
        "        options = tf.data.Options()\n",
        "        options.experimental_deterministic = (False)\n",
        "        ds = ds.with_options(options)\n",
        "\n",
        "    if batch_size:\n",
        "        if teacher_forcing:\n",
        "            ds = ds.padded_batch(batch_size, padding_values=((PAD,0),(0,0)), padded_shapes=(([max_len,CHANNELS],[target_len,]),([target_len,],[target_len,])), drop_remainder=drop_remainder)\n",
        "        else:\n",
        "            ds = ds.padded_batch(batch_size, padding_values=(PAD,0), padded_shapes=([max_len,CHANNELS],[target_len,]), drop_remainder=drop_remainder)\n",
        "\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "ds = get_tfrec_dataset(TRAIN_FILENAMES, train=True, augment=True, batch_size=1024, shuffle=1024)\n",
        "for x in ds:\n",
        "    temp_train = x\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iGLuOeKUCXOe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_dataset_ops import filter_dataset_eager_fallback\n",
        "class ECA(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:,None,:]\n",
        "        return inputs * nn\n",
        "\n",
        "class MaskingDWConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size, strides=1,\n",
        "        dilation_rate=1,\n",
        "        padding='same',\n",
        "        use_bias=False,\n",
        "        kernel_initializer='glorot_uniform',**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        assert padding == 'same' or padding == 'causal'\n",
        "        self.strides = strides\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.use_bias = use_bias\n",
        "        self.padding = padding\n",
        "        self.conv = tf.keras.layers.DepthwiseConv1D(\n",
        "                            kernel_size,\n",
        "                            strides=strides,\n",
        "                            dilation_rate=dilation_rate,\n",
        "                            padding=padding,\n",
        "                            use_bias=use_bias,\n",
        "                            kernel_initializer=kernel_initializer)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "      if mask is not None:\n",
        "        if self.strides > 1:\n",
        "          mask = mask[:,::self.strides]\n",
        "      return mask\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        x = inputs\n",
        "        if mask is not None:\n",
        "            x = tf.where(mask[...,None], x, tf.constant(0., dtype=x.dtype))\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "def Conv1DBlock(channel_size,\n",
        "          kernel_size,\n",
        "          dilation_rate=1,\n",
        "          strides=1,\n",
        "          drop_rate=0.0,\n",
        "          expand_ratio=2,\n",
        "          activation='swish',\n",
        "          name=None):\n",
        "    '''\n",
        "    efficient conv1d block, @hoyso48\n",
        "    '''\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"conv1dblock\"))\n",
        "    # Expansion phase\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'pre_bn')(inputs)\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand,\n",
        "            use_bias=True,\n",
        "            activation=activation,\n",
        "            name=name + '_expand_conv')(x)\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        x = MaskingDWConv1D(kernel_size,\n",
        "            dilation_rate=dilation_rate,\n",
        "            strides=strides,\n",
        "            use_bias=False,\n",
        "            name=name + '_dwconv')(x)\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'conv_bn')(x)\n",
        "\n",
        "        x = ECA()(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channel_size,\n",
        "            use_bias=True,\n",
        "            name=name + '_project_conv')(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
        "\n",
        "        if (channels_in == channel_size) and (strides == 1):\n",
        "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
        "        return x\n",
        "\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yOCCJL9pCXOe"
      },
      "outputs": [],
      "source": [
        "class PosEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=64, max_len=64, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=dim)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, x, positions=None):\n",
        "        if positions is None:\n",
        "            maxlen = tf.shape(x)[1]\n",
        "            positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.q = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.k = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.v = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_causal_mask(self, q, k):\n",
        "        q_len = tf.shape(q)[1]\n",
        "        k_len = tf.shape(k)[1]\n",
        "        i = tf.range(q_len)[:, None]\n",
        "        j = tf.range(k_len)\n",
        "        mask = i >= j\n",
        "        mask = tf.reshape(mask, (q_len, k_len))\n",
        "        return mask\n",
        "\n",
        "    def merge_input_state(self, input, state, layer):\n",
        "        if input is not None and state is not None:\n",
        "            return tf.keras.layers.Concatenate(axis=1)([state, layer(input)])\n",
        "        elif input is not None and state is None:\n",
        "            return layer(input)\n",
        "        elif input is None and state is not None:\n",
        "            return state\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "    def call(self, q, k=None, v=None, key_state=None, value_state=None, return_states=False, use_causal_mask=False):\n",
        "        q = self.q(q)\n",
        "        k = self.merge_input_state(k, key_state, self.k)\n",
        "        v = self.merge_input_state(v, value_state, self.v)\n",
        "        mask = getattr(k, '_keras_mask', None) # we only consider mask from the 'key' here.\n",
        "        if mask is not None:\n",
        "            mask = mask[:,None,None,:]\n",
        "        if use_causal_mask:\n",
        "            if mask is not None:\n",
        "                mask = tf.logical_and(mask, self.get_causal_mask(q,k)[None,None,:,:])\n",
        "            else:\n",
        "                mask = self.get_causal_mask(q,k)[None,None,:,:]\n",
        "        q_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(q))\n",
        "        k_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(k))\n",
        "        v_ = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim // self.num_heads))(v))\n",
        "        attn = tf.matmul(q_, k_, transpose_b=True) * self.scale\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v_\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        if return_states:\n",
        "            return x, k, v\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "def TransformerDecoderBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0., activation='swish', name=None):\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"transformerdecoderblock\"))\n",
        "    def apply(q,k,v):\n",
        "        x = q\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n",
        "        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_self_attn')(x,x,x,use_causal_mask=True)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop1')(x)\n",
        "        x = tf.keras.layers.Add(name=name + '_add1')([q, x])\n",
        "        attn_out1 = x\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn2')(x)\n",
        "        x = MultiHeadAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_cross_attn')(x,k,v)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop2')(x)\n",
        "        x = tf.keras.layers.Add(name=name + '_add2')([attn_out1, x])\n",
        "        attn_out2 = x\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn3')(x)\n",
        "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop3')(x)\n",
        "        x = tf.keras.layers.Add(name=name + '_add3')([attn_out2, x])\n",
        "        return x\n",
        "    return apply\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish', name=None):\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"transformerblock\"))\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn1')(x)\n",
        "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout, name=name + '_mhsa')(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop1')(x)\n",
        "        x = tf.keras.layers.Add(name=name + '_add1')([inputs, x])\n",
        "        attn_out = x\n",
        "\n",
        "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + 'bn2')(x)\n",
        "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation, name=name + '_fc1')(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False, name=name + '_fc2')(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop2')(x)\n",
        "        x = tf.keras.layers.Add(name=name + '_add2')([attn_out, x])\n",
        "        return x\n",
        "    return apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0zW1_dgqCXOe"
      },
      "outputs": [],
      "source": [
        "class CTCLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, blank_index=0, input_padding_value=0., target_padding_value=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.blank_index = blank_index\n",
        "        self.input_padding_value = input_padding_value\n",
        "        self.target_padding_value = target_padding_value\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=tf.int32)\n",
        "        label_length = y_true != tf.cast(self.target_padding_value, tf.int32)\n",
        "        label_length = tf.reduce_sum(tf.cast(label_length, tf.int32), axis=1, keepdims=False) #(B,)\n",
        "        mask = getattr(y_pred, '_keras_mask', None)\n",
        "        if mask is not None:\n",
        "            input_length = tf.reduce_sum(tf.cast(mask, tf.int32), axis=-1)\n",
        "        else:\n",
        "            input_length = tf.cast(tf.shape(y_pred)[1], dtype=tf.int32)\n",
        "            input_length = input_length * tf.ones(shape=(batch_len,), dtype=tf.int32)\n",
        "\n",
        "        loss = tf.nn.ctc_loss(y_true, y_pred, label_length=label_length, logit_length=input_length, blank_index=0, logits_time_major=False)\n",
        "\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class MaskedSCCE(tf.keras.losses.Loss):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, from_logits=True, label_smoothing=0.25, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.label_smoothing=label_smoothing\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        mask = y_true!=0\n",
        "        N = tf.shape(y_true)[0]\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        y_true = tf.one_hot(y_true, self.num_classes, axis=-1, dtype=tf.float32)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=self.from_logits, label_smoothing=self.label_smoothing)\n",
        "        loss = tf.where(mask, loss, tf.constant(0, dtype=tf.float32))\n",
        "        loss = tf.reduce_sum(loss)\n",
        "        loss = loss / tf.cast(N, tf.float32)\n",
        "        return loss\n",
        "\n",
        "class Accuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Accuracy, self).__init__(name=f'acc', **kwargs)\n",
        "        self.acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.reshape(y_true, [-1])\n",
        "        y_pred = tf.reshape(y_pred, [-1, tf.shape(y_pred)[-1]])\n",
        "        mask = y_true != 0\n",
        "        y_true = tf.boolean_mask(y_true, mask)\n",
        "        y_pred = tf.boolean_mask(y_pred, mask)\n",
        "        self.acc.update_state(y_true, y_pred)\n",
        "\n",
        "    def result(self):\n",
        "        return self.acc.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.acc.reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hr_6n-QWCXOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "059ebc05-df7b-40bb-cfb4-79b3be3d75fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 1629)]          0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " encoder (Functional)        (None, 64, 192)              5565377   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " att_decoder (Functional)    (None, None, 62)             480830    ['input_3[0][0]',             \n",
            "                                                                     'encoder[0][0]']             \n",
            "                                                                                                  \n",
            " ctc_decoder (Functional)    (None, 64, 62)               320318    ['encoder[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6366525 (24.29 MB)\n",
            "Trainable params: 6336957 (24.17 MB)\n",
            "Non-trainable params: 29568 (115.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def get_model(max_len=128, target_len=64, dim=192, dtype='float32'):\n",
        "    ################# ENCODER #################\n",
        "    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n",
        "    x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp1)\n",
        "    ksize = 17\n",
        "    drop_rate = 0.2\n",
        "    x = tf.keras.layers.Dense(dim,use_bias=False,name='stem_conv')(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=0,strides=2)(x) #drop_rate=0 since we don't want to drop the whole output here\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim,ksize,expand_ratio=4,drop_rate=drop_rate)(x)\n",
        "    x = TransformerBlock(dim,expand=2,num_heads=4,drop_rate=drop_rate,attn_dropout=0.2)(x)\n",
        "    x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
        "\n",
        "    encoder = tf.keras.Model(inp1,x,name='encoder')\n",
        "\n",
        "    ################# CTC DECDODER #################\n",
        "    inp3 = tf.keras.Input((x.shape[1],dim),name='ctc_decoder_inp2',dtype=dtype)\n",
        "    x = inp3\n",
        "    x = tf.keras.layers.RNN(tf.keras.layers.GRUCell(dim), return_sequences=True)(x)\n",
        "    x = tf.keras.layers.Dense(dim*2)(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    x = tf.keras.layers.Dense(NUM_CLASSES,name='ctc_classifier')(x) #include sos, eos token\n",
        "    ctc_decoder = tf.keras.Model(inp3,x,name='ctc_decoder')\n",
        "\n",
        "    ################# ATT DECODER #################\n",
        "    inp2 = tf.keras.Input((None,),name='att_decoder_inp1',dtype='int32')\n",
        "    inp3 = tf.keras.Input((x.shape[1],dim),name='att_decoder_inp2',dtype=dtype)\n",
        "\n",
        "    x = inp3\n",
        "    y = tf.keras.layers.Masking(mask_value=0,input_shape=(None,),name='att_decoder_input_masking')(inp2)\n",
        "    y = tf.keras.layers.Embedding(NUM_CLASSES,dim,mask_zero=True,name='att_decoder_token_emb')(y) #include sos token\n",
        "    y = PosEmbedding(dim,max_len=target_len,name='att_decoder_pos_emb')(y)\n",
        "    y = TransformerDecoderBlock(dim,expand=2,num_heads=4,attn_dropout=0.2,name='att_decoder_block1')(y,x,x)\n",
        "    y = tf.keras.layers.Dropout(0.5)(y)\n",
        "    y = tf.keras.layers.Dense(NUM_CLASSES,name='att_decoder_classifier')(y)\n",
        "\n",
        "    decoder = tf.keras.Model([inp2,inp3],y,name='att_decoder')\n",
        "\n",
        "    ################### MODEL #####################\n",
        "    inp1 = tf.keras.Input((max_len,CHANNELS),dtype=dtype)\n",
        "    inp2 = tf.keras.Input((None,),dtype='int32')\n",
        "\n",
        "    x = inp1\n",
        "    enc_out = encoder(x)\n",
        "    y = inp2\n",
        "    dec_out = decoder([y, enc_out])\n",
        "    ctc_out = ctc_decoder(enc_out)\n",
        "    model = tf.keras.Model([inp1,inp2], [dec_out,ctc_out])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "y = model(temp_train[0], training=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MwWfaKK_CXOf"
      },
      "outputs": [],
      "source": [
        "#check supports_masking\n",
        "for x in model.layers:\n",
        "    if not x.supports_masking:\n",
        "        print(x.supports_masking, x.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cSSYj3tBCXOf"
      },
      "outputs": [],
      "source": [
        "class GreedyDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, model, max_output_length=64, sos_token_idx=60, eos_token_idx=61, pad_token_idx=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.encoder = self.model.get_layer('encoder')\n",
        "        self.decoder = self.model.get_layer('att_decoder')\n",
        "        self.inference_module = self.model.get_layer('att_decoder')\n",
        "        self.max_output_length = max_output_length\n",
        "        self.sos_token_idx = sos_token_idx\n",
        "        self.eos_token_idx = eos_token_idx\n",
        "        self.pad_token_idx = pad_token_idx\n",
        "\n",
        "    def call(self, batch_x):\n",
        "        encoder_out = self.encoder(batch_x)\n",
        "\n",
        "        time = tf.constant(0, dtype=tf.int32)\n",
        "        predictions = tf.ones((tf.shape(batch_x)[0],1), dtype=tf.int32) * self.sos_token_idx\n",
        "        pad = tf.ones((tf.shape(batch_x)[0],), dtype=tf.int32) * self.pad_token_idx\n",
        "        init = True\n",
        "\n",
        "        def condition(_time, _predictions):\n",
        "            return tf.logical_and(_time < self.max_output_length, tf.logical_not(tf.reduce_all(tf.reduce_any(_predictions==self.eos_token_idx, axis=1))))\n",
        "\n",
        "        def body(_time, _predictions):\n",
        "            out = self.inference_module([_predictions, encoder_out])\n",
        "            pred_curr = tf.where(tf.reduce_any(_predictions==self.eos_token_idx, axis=1), [self.pad_token_idx], tf.argmax(out[:,-1], axis=-1, output_type=tf.int32))\n",
        "            _predictions = tf.concat([_predictions, pred_curr[...,None]], axis=1)\n",
        "            return _time+1, _predictions\n",
        "\n",
        "        _, predictions = tf.while_loop(condition, body, loop_vars=[time, predictions])\n",
        "        return predictions[:,1:]\n",
        "\n",
        "class KerasCTCDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, model, greedy=True, beam_width=100, from_logits=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model = model\n",
        "        self.greedy = greedy\n",
        "        self.beam_width = beam_width\n",
        "        self.from_logits = from_logits\n",
        "        self.encoder = self.model.get_layer('encoder')\n",
        "        self.ctc_decoder = self.model.get_layer('ctc_decoder')\n",
        "\n",
        "    def call(self, batch_x):\n",
        "        encoder_out = self.encoder(batch_x)\n",
        "        input_length = tf.reduce_sum(tf.cast(encoder_out._keras_mask, tf.int32), axis=1)\n",
        "        predictions = self.ctc_decoder(encoder_out)\n",
        "        if not self.greedy and self.from_logits:\n",
        "            predictions = tf.nn.softmax(predictions, axis=-1)\n",
        "        predictions = tf.keras.backend.ctc_decode(tf.cast(predictions, tf.float32), input_length=input_length, greedy=self.greedy, beam_width=self.beam_width)[0][0]\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "A9c-6-24CXOf"
      },
      "outputs": [],
      "source": [
        "def make_predictions(recognizer, ds):\n",
        "    results = []\n",
        "    for batch in tqdm(ds):\n",
        "        result = recognizer(batch[0][0])\n",
        "        results.append(num_to_char(result.numpy()))\n",
        "    results = np.array([item for sublist in results for item in sublist])\n",
        "    return results\n",
        "\n",
        "def num_to_char(list_of_nums, n2c_dict=NUM_TO_CHAR):\n",
        "    def n_to_c(x):\n",
        "        return [n2c_dict[a] for a in x if a!=-1]\n",
        "    char_list = [''.join(n_to_c(x)).replace('P','').replace('S','').replace('E','') for x in list_of_nums]\n",
        "    return np.array(char_list, dtype='str')\n",
        "\n",
        "def extract_labels(ds):\n",
        "    labels = [num_to_char(x[1][0].numpy()) for x in ds]\n",
        "    labels = np.array([item for sublist in labels for item in sublist])\n",
        "    return labels\n",
        "\n",
        "from Levenshtein import distance\n",
        "def competition_metric(true, pred):\n",
        "    #true: list of strings, ground truths\n",
        "    #pred: list of strings, predictions\n",
        "    D = sum([distance(x,y) for x,y in zip(true, pred)])\n",
        "    N = len(''.join(true))\n",
        "    return max((N-D)/N, 0.), D/len(true)\n",
        "\n",
        "def display(labels, preds):\n",
        "    for target,prediction in zip(labels, preds):\n",
        "        print(f\"Target    : {target}\")\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "        print(\"-\" * 100)\n",
        "    return\n",
        "\n",
        "def evaluate(model, ds, labels=None, display_index='random', num_display=5):\n",
        "    if labels is None:\n",
        "        labels = extract_labels(ds)\n",
        "    preds = make_predictions(model, ds)\n",
        "    score, mean_dist = competition_metric(labels, preds)\n",
        "    num_display = min(len(labels), num_display)\n",
        "    if display_index=='random':\n",
        "        if num_display:\n",
        "            idxs = np.random.choice(range(len(labels)),num_display,replace=False)\n",
        "            display(labels[idxs], preds[idxs])\n",
        "    elif display_index=='init':\n",
        "        if num_display:\n",
        "            display(labels[:num_display], preds[:num_display])\n",
        "    elif isinstance(display_index, list):\n",
        "        if display_index:\n",
        "            display(labels[display_index], preds[display_index])\n",
        "    else:\n",
        "        pass\n",
        "    print(f'Score: {score:0.4f}')\n",
        "    print(f'mean_dist: {mean_dist:0.4f}')\n",
        "    # return labels, preds, score\n",
        "    del preds, score\n",
        "    return\n",
        "\n",
        "class Eval(tf.keras.callbacks.Callback):\n",
        "    def __init__(self,recognizer,ds,labels=None,eval_epochs=[],display_index='random',num_display=5):\n",
        "        super().__init__()\n",
        "        self.recognizer = recognizer\n",
        "        self.ds = ds\n",
        "        self.labels = labels\n",
        "        self.eval_epochs = eval_epochs\n",
        "        self.display_index = display_index\n",
        "        self.num_display = num_display\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch in self.eval_epochs and self.ds is not None: # your custom condition\n",
        "            evaluate(self.recognizer, self.ds, self.labels, self.display_index, self.num_display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zsuK9Cx6CXOf"
      },
      "outputs": [],
      "source": [
        "class AWP(tf.keras.Model):\n",
        "    def __init__(self, *args, lr=0.1, eps=1e-6, start_step=0, exclude=[], **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.start_step = start_step\n",
        "        self.exclude = exclude\n",
        "\n",
        "    def compute_perturbation(self, param, param_gradient):\n",
        "        grad = tf.zeros_like(param) + param_gradient\n",
        "        #delta = tf.math.divide_no_nan(self.lr * grad * tf.norm(param), tf.norm(grad) + self.eps) #original implemenation from the paper\n",
        "        delta = tf.math.divide_no_nan(self.lr * grad, tf.norm(grad) + self.eps)\n",
        "        return delta\n",
        "\n",
        "    def train_step_awp(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        params = self.trainable_variables\n",
        "        params_gradients = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        for i in range(len(params_gradients)):\n",
        "            if not any(s in params[i].name for s in self.exclude):\n",
        "                delta = self.compute_perturbation(params[i], params_gradients[i])\n",
        "                self.trainable_variables[i].assign_add(delta)\n",
        "\n",
        "        with tf.GradientTape() as tape2:\n",
        "            y_pred = self(x, training=True)\n",
        "            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "            if hasattr(self.optimizer, 'get_scaled_loss'):\n",
        "                new_loss = self.optimizer.get_scaled_loss(new_loss)\n",
        "\n",
        "        gradients = tape2.gradient(new_loss, self.trainable_variables)\n",
        "        if hasattr(self.optimizer, 'get_unscaled_gradients'):\n",
        "            gradients =  self.optimizer.get_unscaled_gradients(gradients)\n",
        "\n",
        "        for i in range(len(params_gradients)):\n",
        "            if not any(s in params[i].name for s in self.exclude):\n",
        "                delta = self.compute_perturbation(params[i], params_gradients[i])\n",
        "                self.trainable_variables[i].assign_sub(delta)\n",
        "\n",
        "        #if nan is detected, skip update\n",
        "        # nan_detected = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients])\n",
        "        # _ = tf.cond(nan_detected, lambda:tf.constant(False),lambda:self.optimizer.apply_gradients(zip(gradients, self.trainable_variables)))\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def train_step(self, data):\n",
        "        return tf.cond(self._train_counter < self.start_step, lambda:super(AWP,self).train_step(data), lambda:self.train_step_awp(data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CSVLoggerV2(tf.keras.callbacks.CSVLogger):\n",
        "    def __init__(self, filename, separator=\",\", resume=0):\n",
        "        self.resume = resume\n",
        "        super().__init__(filename=filename, separator=separator, append=bool(resume))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        super(CSVLoggerV2,self).on_epoch_end(epoch=epoch+self.resume+1, logs=logs)"
      ],
      "metadata": {
        "id": "kooMcNeDH1k8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B44t6dsLCXOf"
      },
      "outputs": [],
      "source": [
        "def train_fold(CFG, fold, train_files, valid_files=None, strategy=STRATEGY, summary=True):\n",
        "    seed_everything(CFG.seed)\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    # tf.config.optimizer.set_jit(True)\n",
        "\n",
        "    policy = mixed_precision.Policy(CFG.policy)\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "\n",
        "    if CFG.resume == 'auto':\n",
        "        if os.path.isfile(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv'):\n",
        "            resume = pd.read_csv(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')['epoch'].values[-1]\n",
        "            resume = 0 if resume == CFG.epoch else resume #restart if training is already fininshed\n",
        "        else:\n",
        "            resume = 0\n",
        "    else:\n",
        "        resume = CFG.resume\n",
        "\n",
        "    if fold != 'all':\n",
        "        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.train_batch_size, max_len=CFG.max_len, drop_remainder=True, train=True, augment=True, repeat=True, shuffle=4096)\n",
        "        valid_ds = get_tfrec_dataset(valid_files, batch_size=CFG.valid_batch_size, max_len=CFG.max_len, drop_remainder=True, repeat=False, shuffle=False)\n",
        "    else:\n",
        "        train_ds = get_tfrec_dataset(train_files, batch_size=CFG.train_batch_size, max_len=CFG.max_len, drop_remainder=True, train=True, augment=True, repeat=True, shuffle=4096)\n",
        "        valid_ds = None\n",
        "        valid_files = []\n",
        "\n",
        "    num_train = count_data_items(train_files)\n",
        "    num_valid = count_data_items(valid_files)\n",
        "    steps_per_epoch = num_train//CFG.train_batch_size\n",
        "    with strategy.scope():\n",
        "        model = get_model(max_len=CFG.max_len, dim=CFG.dim, dtype='bfloat16') #dtype should be matched with CFG.policy\n",
        "\n",
        "        schedule = OneCycleLR(CFG.lr, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min, decay_type=CFG.decay_type, warmup_type=CFG.warmup_type)\n",
        "        decay_schedule = OneCycleLR(CFG.lr*CFG.weight_decay, CFG.epoch, warmup_epochs=CFG.epoch*CFG.warmup, steps_per_epoch=steps_per_epoch, resume_epoch=resume, decay_epochs=CFG.epoch, lr_min=CFG.lr_min*CFG.weight_decay, decay_type=CFG.decay_type, warmup_type=CFG.warmup_type)\n",
        "\n",
        "        awp_start_epoch = max(CFG.awp_start_epoch - resume, 0)\n",
        "        awp_step = int(awp_start_epoch * steps_per_epoch)\n",
        "        if CFG.fgm:\n",
        "            model = FGM(model.input, model.output, lr=CFG.awp_lr, eps=0., start_step=awp_step)\n",
        "        elif CFG.awp:\n",
        "            model = AWP(model.input, model.output, lr=CFG.awp_lr, eps=0., start_step=awp_step, exclude=['bias','gamma','beta','rnn'])\n",
        "\n",
        "        # opt = tfa.optimizers.RectifiedAdam(learning_rate=schedule, weight_decay=decay_schedule, sma_threshold=4)\n",
        "        # opt = tfa.optimizers.Lookahead(opt,sync_period=5)\n",
        "        opt = tfa.optimizers.AdamW(learning_rate=schedule, weight_decay=decay_schedule)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=opt,\n",
        "            loss=[MaskedSCCE(label_smoothing=0.25), CTCLoss()],\n",
        "            loss_weights=[0.75,0.25],\n",
        "            metrics=[\n",
        "                [\n",
        "                Accuracy(),\n",
        "                ],\n",
        "                [],\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    if summary:\n",
        "        print()\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(train_ds, valid_ds)\n",
        "        print()\n",
        "        schedule.plot()\n",
        "        print()\n",
        "        init=False\n",
        "    print(f'---------fold{fold}---------')\n",
        "    print(f'train:{num_train} valid:{num_valid}')\n",
        "    print()\n",
        "\n",
        "    if resume:\n",
        "        print(f'resume from epoch{resume}')\n",
        "        if CFG.resume_ckpt:\n",
        "            print(f'load weights from {CFG.resume_ckpt}')\n",
        "            model.load_weights(CFG.resume_ckpt)\n",
        "        else:\n",
        "            print(f'load weights from {CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n",
        "            model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n",
        "        # if train_ds is not None:\n",
        "        #     model.evaluate(train_ds.take(steps_per_epoch))\n",
        "        # if valid_ds is not None:\n",
        "        #     model.evaluate(valid_ds)\n",
        "\n",
        "    logger = CSVLoggerV2(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv', resume=resume)\n",
        "\n",
        "    mode = 'min'\n",
        "    if fold != 'all':\n",
        "        monitor = 'val_loss'\n",
        "    else:\n",
        "        monitor = 'loss'\n",
        "    if resume:\n",
        "        prev_best = pd.read_csv(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-logs.csv')[monitor].agg(mode)\n",
        "    else:\n",
        "        prev_best = None\n",
        "    sv_loss = tf.keras.callbacks.ModelCheckpoint(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-best.h5', monitor=monitor, verbose=0, save_best_only=True,\n",
        "                  save_weights_only=True, mode='min', save_freq='epoch', initial_value_threshold=prev_best)\n",
        "\n",
        "    snap = Snapshot(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', snapshot_epochs=[])\n",
        "    # swa = SWA(f'{CFG.output_dir}/{CFG.comment}-fold{fold}', CFG.swa_epochs, strategy=strategy, train_ds=train_ds, valid_ds=valid_ds)\n",
        "\n",
        "    callbacks = []\n",
        "    if CFG.save_output:\n",
        "        callbacks.append(logger)\n",
        "        callbacks.append(snap)\n",
        "        # callbacks.append(swa)\n",
        "        callbacks.append(sv_loss)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=CFG.epoch-resume,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=valid_ds,\n",
        "        verbose=CFG.verbose,\n",
        "    )\n",
        "\n",
        "    if fold != 'all':\n",
        "        ds = get_tfrec_dataset(valid_files, batch_size=CFG.valid_batch_size, max_len=CFG.max_len, drop_remainder=False, repeat=False, shuffle=False)\n",
        "        labels = extract_labels(ds)\n",
        "        model.load_weights(f'{CFG.output_dir}/{CFG.comment}-fold{fold}-last.h5')\n",
        "        print('ATTENTION EVAL')\n",
        "        evaluate(GreedyDecoder(model),ds,labels)\n",
        "        print()\n",
        "        print('CTC EVAL')\n",
        "        evaluate(KerasCTCDecoder(model, greedy=True),ds,labels)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def train_folds(CFG, folds, strategy=STRATEGY, summary=True):\n",
        "    for fold in folds:\n",
        "        if fold != 'all':\n",
        "            all_files = TRAIN_FILENAMES\n",
        "            train_files = [x for x in all_files if f'fold{fold}' not in x]\n",
        "            valid_files = [x for x in all_files if f'fold{fold}' in x]\n",
        "        else:\n",
        "            train_files = TRAIN_FILENAMES\n",
        "            valid_files = None\n",
        "\n",
        "        train_fold(CFG, fold, train_files, valid_files, strategy=strategy, summary=summary)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iOV1SiNOCXOf"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    n_splits = 5\n",
        "    save_output = True\n",
        "    output_dir = '.'\n",
        "\n",
        "    seed = 42\n",
        "    verbose = 'auto' #0) silent 1) progress bar 2) one line per epoch\n",
        "\n",
        "    dim = 192\n",
        "    max_len = 768\n",
        "\n",
        "    policy = 'mixed_bfloat16' #'float32') fp32, 'mixed_float16') GPU+fp16, 'mixed_bfloat16') TPU+fp16\n",
        "    replicas = N_REPLICAS\n",
        "    lr = 5e-4 * replicas\n",
        "    weight_decay = 0.01\n",
        "    lr_min = 1e-6\n",
        "    epoch = 400 #200\n",
        "    warmup = 0.1\n",
        "    warmup_type = 'linear'\n",
        "    decay_type = 'cosine'\n",
        "    train_batch_size = 16 * replicas\n",
        "    valid_batch_size = 64 * replicas\n",
        "\n",
        "    fgm = False\n",
        "    awp = True\n",
        "    awp_lr = 0.2 #0.1\n",
        "    awp_start_epoch = 0.1 * epoch\n",
        "\n",
        "    resume = 0\n",
        "    resume_ckpt = ''\n",
        "    comment =  f'aslfr-fp16-192d-17l-ctcattjoint-seed{seed}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FufKMDdaCXOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7c1f4380c8274391a8e5126b9d06bc3b",
            "c72f475d6bc34d5d81354325dbab8b8b",
            "a0b5e5c208f744a080d3d4168c55e1e4",
            "68449d84fe3840489aeeab335715be73",
            "bffbca07e97e4aab811b0047df570c1d",
            "9368558ad100475391fb4e14af21da1a",
            "b79316e2a4f94cffb8803371f616c355",
            "50ef7bd0303f44eab4bd72062e39be6e",
            "51a6f3eb0ed54cb9a7794fafd1196cf8",
            "ff63ef99b0754652b56ff823958d84f5",
            "05cd74b3223645dd88bcb0274652c208",
            "65428bcbcbec4928aa1631b1badd2a9d",
            "94b5dbfb25744367ad25bb78098f3719",
            "65949a79e796428d84c63a8c60393280",
            "1fb10d27f1c940b3a54d13e04dd9fc4a",
            "841e0d4e9dae4eb2ba9169aa6da23dbc",
            "6d0920242efb48beb80c606e54f36355",
            "f13e102e46fb403fbf244f16891fdb0b",
            "0b3435a6413745e198b6d47d31e534ff",
            "cd99ce4202294ca7a3ad6fc827108020",
            "9a7df609d74b4109b9c921fa57cb036f",
            "c3278b176f774f50beb331b2721c7c46"
          ]
        },
        "outputId": "3e0f7500-180c-45de-aa97-e724b69783ce"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"awp\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 768, 1629)]          0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " encoder (Functional)        (None, 384, 192)             5565377   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " att_decoder (Functional)    (None, None, 62)             480830    ['input_3[0][0]',             \n",
            "                                                                     'encoder[0][0]']             \n",
            "                                                                                                  \n",
            " ctc_decoder (Functional)    (None, 384, 62)              320318    ['encoder[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6366525 (24.29 MB)\n",
            "Trainable params: 6336957 (24.17 MB)\n",
            "Non-trainable params: 29568 (115.50 KB)\n",
            "__________________________________________________________________________________________________\n",
            "\n",
            "<_PrefetchDataset element_spec=((TensorSpec(shape=(128, 768, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(128, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(128, 64), dtype=tf.int32, name=None), TensorSpec(shape=(128, 64), dtype=tf.int32, name=None)))> <_PrefetchDataset element_spec=((TensorSpec(shape=(512, 768, 1629), dtype=tf.float32, name=None), TensorSpec(shape=(512, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(512, 64), dtype=tf.int32, name=None), TensorSpec(shape=(512, 64), dtype=tf.int32, name=None)))>\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAGdCAYAAADUl+3IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3OUlEQVR4nO3de3RU9b3//1cmQzLhkolpZJLBALEi10CUyxCOymmZ0yC0Na3ra6AsQcqB1orFA1TACvR4XCeIWv2i1GjXsXR9iwVZtbQ/5OQcDFJUYoAAAoIpWCooJogxExIJJJnP7w+bDQPhMuQyt+djrVlh9n7vzGc+bpiXsz+fz44zxhgBAADgmtlC3QAAAIBIR6ACAABoIwIVAABAGxGoAAAA2ohABQAA0EYEKgAAgDYiUAEAALQRgQoAAKCN7KFuQDjx+/06fvy4evToobi4uFA3BwAAXAVjjE6dOiW32y2bLTTfFRGoznP8+HFlZmaGuhkAAOAaHDt2TDfccENIXptAdZ4ePXpI+uo/SHJycohbAwAArkZtba0yMzOtz/FQIFCdp+UyX3JyMoEKAIAIE8rhOgxKBwAAaCMCFQAAQBsRqAAAANqIQAUAANBGBCoAAIA2IlABAAC0EYEKAACgjQhUAAAAbUSgAgAAaCMCFQAAQBsRqAAAANqIQNVJmpr9+ttndWpq9oe6KQAAoJ1xc+RO0NTs1/d/tU17P/FpaC+nXvvJGNnjybIAAEQLPtU7wdHqL7X3E58kae8nPh2t/jLELQIAAO2JQNUJeqd21dBeTknS0Buc6p3aNcQtAgAA7YlLfp3AHm/Tqz8arbIj1fJkpXK5DwCAKEOg6gRNzX7d8+K7jKECACBK8aneCRhDBQBAdCNQdYLeqV2V7U6WJGX3SmYMFQAAUYZA1Vni4lr+ENJmAACA9keg6gRHq7/Uvn9c8tvHJT8AAKIOgaoTnL9sQn9Xd7mdjhC3CAAAtCcCVSdoWTahv6u7KqrqdM+L73ILGgAAogiBqpMc9zWooqpOEjP9AACINgSqTsJq6QAARC8W9uwkrJYOAED0IlB1ElZLBwAgevGJ3klYLR0AgOhFoOokLJ0AAED0IlB1EpZOAAAgehGoOhFLJwAAEJ0IVJ2ImyQDABCdCFSdjZskAwAQdQhUnYibJAMAEJ0IVJ2ImX4AAEQnAlUnYqYfAADRiUDVyZjpBwBA9CFQdTJukgwAQPThXn6djJskAwAQfQhUnYybJAMAEH34JO9k3CQZAIDoQ6DqZCydAABA9CFQdTKWTgAAIPoQqEKApRMAAIgu1xSoVq5cqb59+8rhcMjj8Wj79u2XrV+3bp0GDBggh8Oh7Oxsbdy4MWC/MUZLlixRRkaGkpKS5PV6dejQoYCa6upqTZkyRcnJyUpJSdGMGTNUV1fX6usdPnxYPXr0UEpKyrW8vQ7HTZIBAIguQQeqtWvXau7cuVq6dKl27dqlYcOGKS8vTydOnGi1ftu2bZo8ebJmzJih3bt3Kz8/X/n5+dq/f79Vs3z5cq1YsUJFRUUqKytTt27dlJeXp4aGBqtmypQpev/997Vp0yZt2LBBW7du1axZsy56vcbGRk2ePFm33357sG+tc3GTZAAAokacMcYEc4DH49HIkSP1/PPPS5L8fr8yMzP14IMPauHChRfVFxQUqL6+Xhs2bLC2jR49Wjk5OSoqKpIxRm63W/PmzdP8+fMlST6fTy6XS6tWrdKkSZN08OBBDRo0SDt27NCIESMkScXFxZowYYI+/vhjud1u63cvWLBAx48f17hx4/TQQw+ppqbmqt9bbW2tnE6nfD6fkpOTg+mWoPztszp98+m/WM83zxurG6/v3mGvBwBANOusz+/LCeobqrNnz6q8vFxer/fcL7DZ5PV6VVpa2uoxpaWlAfWSlJeXZ9UfOXJElZWVATVOp1Mej8eqKS0tVUpKihWmJMnr9cpms6msrMzatnnzZq1bt04rV64M5m11Oi75AQAQXYJa2PPkyZNqbm6Wy+UK2O5yufTBBx+0ekxlZWWr9ZWVldb+lm2Xq+nZs2dgw+12paamWjWff/657rvvPv3ud7+76nR65swZnTlzxnpeW1t7Vce1Cy75AQAQNaJmlt/MmTP1gx/8QHfcccdVH1NYWCin02k9MjMzO7CF5xyt/lL7/rG45z5m+QEAEPGCClRpaWmKj49XVVVVwPaqqiqlp6e3ekx6evpl61t+XqnmwkHvTU1Nqq6utmo2b96sp556Sna7XXa7XTNmzJDP55PdbtfLL7/catsWLVokn89nPY4dO3Y13dBmLO4JAEB0CSpQJSQkaPjw4SopKbG2+f1+lZSUKDc3t9VjcnNzA+oladOmTVZ9VlaW0tPTA2pqa2tVVlZm1eTm5qqmpkbl5eVWzebNm+X3++XxeCR9Nc5qz5491uOxxx5Tjx49tGfPHn3ve99rtW2JiYlKTk4OeHQGFvcEACC6BH1z5Llz52ratGkaMWKERo0apWeffVb19fWaPn26JGnq1Knq1auXCgsLJUlz5szR2LFj9fTTT2vixIlas2aNdu7cqZdeekmSFBcXp4ceekiPP/64+vXrp6ysLC1evFhut1v5+fmSpIEDB2r8+PGaOXOmioqK1NjYqNmzZ2vSpEnWDL+BAwcGtHPnzp2y2WwaMmTINXdOR2ptcU9m+gEAEJmCDlQFBQX67LPPtGTJElVWVionJ0fFxcXWoPKjR4/KZjv3xdeYMWP0yiuv6NFHH9Ujjzyifv36af369QFB5+GHH1Z9fb1mzZqlmpoa3XbbbSouLpbDce5S2OrVqzV79myNGzdONptNd999t1asWNGW9x5SLTP99h2vZaYfAAARLuh1qKJZZ65j0dTs1/d+tU37PvEpu5dTf/zJGNnjo2aOAAAAnSbi1qFC+2GmHwAA0YNAFSLM9AMAIHoQqEKEmX4AAEQPAlUItTbTDwAARB4CVQidf9lv6A1OZvoBABChgl42Ae2n5bJf2ZFqebJSmeUHAECEIlCFUFOzX/e8+K72fuLT0F5OvcbSCQAARCQ+vUPoaPWX2vuPpRMYQwUAQOQiUIUQSycAABAdCFQhxNIJAABEBwJViLF0AgAAkY9AFWItN0mWxE2SAQCIUASqcBAX1/KHkDYDAABcGwJViHGTZAAAIh+BKsSY6QcAQOQjUIUYM/0AAIh8BKowwEw/AAAiG4EqDDDTDwCAyEagChfM9AMAIGIRqMIAM/0AAIhsBKowwEw/AAAiG4EqDDDTDwCAyEagChPM9AMAIHIRqMIEM/0AAIhcBKpwwkw/AAAiEoEqTDDTDwCAyEWgChNc8gMAIHIRqMIJl/wAAIhIBKowwSU/AAAiF4EqTLC4JwAAkYtAFSZY3BMAgMhFoAojLO4JAEBkIlCFEWb6AQAQmQhU4YaZfgAARBwCVRhhph8AAJGJQBVGmOkHAEBkIlCFEWb6AQAQmQhUYYaZfgAARB4CVZhhph8AAJGHQBWOmOkHAEBEIVCFGWb6AQAQeQhUYYaZfgAARB4CVZhhph8AAJGHQBWGmOkHAEBkIVCFIbfToaQu8ZKkpC7xXPYDACDMEajC0HFfg043NkuSTjc267ivIcQtAgAAl0OgCkMMTAcAILIQqMIQA9MBAIgsBKowxcB0AAAiB4EqTHELGgAAIgeBKpxxCxoAACICgSpMcQsaAAAiB4EqTDHTDwCAyEGgClPM9AMAIHIQqMIYM/0AAIgMBKowxkw/AAAiA4Eq3DHTDwCAsEegCmPM9AMAIDIQqMIYl/wAAIgMBKpwxyU/AADCHoEqjHHJDwCAyECgCmMs7gkAQGQgUIUxFvcEACAyEKjCHIt7AgAQ/ghUYc7tdCipS7wkKalLPJf9AAAIQ9cUqFauXKm+ffvK4XDI4/Fo+/btl61ft26dBgwYIIfDoezsbG3cuDFgvzFGS5YsUUZGhpKSkuT1enXo0KGAmurqak2ZMkXJyclKSUnRjBkzVFdXZ+2vqKjQN77xDblcLjkcDt1444169NFH1djYeC1vMWwc9zXodGOzJOl0Y7OO+xpC3CIAAHChoAPV2rVrNXfuXC1dulS7du3SsGHDlJeXpxMnTrRav23bNk2ePFkzZszQ7t27lZ+fr/z8fO3fv9+qWb58uVasWKGioiKVlZWpW7duysvLU0PDufAwZcoUvf/++9q0aZM2bNigrVu3atasWdb+Ll26aOrUqfrf//1fVVRU6Nlnn9Wvf/1rLV26NNi3GFYYmA4AQPiLM8aYYA7weDwaOXKknn/+eUmS3+9XZmamHnzwQS1cuPCi+oKCAtXX12vDhg3WttGjRysnJ0dFRUUyxsjtdmvevHmaP3++JMnn88nlcmnVqlWaNGmSDh48qEGDBmnHjh0aMWKEJKm4uFgTJkzQxx9/LLfb3Wpb586dqx07duitt966qvdWW1srp9Mpn8+n5OTkYLqlQzWcbdJdK99RRVWdhvZy6rWfjJE9nqu1AABI4fH5HdSn8tmzZ1VeXi6v13vuF9hs8nq9Ki0tbfWY0tLSgHpJysvLs+qPHDmiysrKgBqn0ymPx2PVlJaWKiUlxQpTkuT1emWz2VRWVtbq6x4+fFjFxcUaO3bsJd/PmTNnVFtbG/AIRwxMBwAgvAUVqE6ePKnm5ma5XK6A7S6XS5WVla0eU1lZedn6lp9XqunZs2fAfrvdrtTU1Ited8yYMXI4HOrXr59uv/12PfbYY5d8P4WFhXI6ndYjMzPzkrWhxC1oAAAIb1F33Wjt2rXatWuXXnnlFb3++ut66qmnLlm7aNEi+Xw+63Hs2LFObGmQuAUNAABhyx5McVpamuLj41VVVRWwvaqqSunp6a0ek56eftn6lp9VVVXKyMgIqMnJybFqLhz03tTUpOrq6otet+VbpkGDBqm5uVmzZs3SvHnzFB8ff1HbEhMTlZiYeKW3HXKt3YLmxuu7h7hVAACgRVDfUCUkJGj48OEqKSmxtvn9fpWUlCg3N7fVY3JzcwPqJWnTpk1WfVZWltLT0wNqamtrVVZWZtXk5uaqpqZG5eXlVs3mzZvl9/vl8Xgu2V6/36/Gxkb5/ZG9ujgz/QAACG9BfUMlfTVzbtq0aRoxYoRGjRqlZ599VvX19Zo+fbokaerUqerVq5cKCwslSXPmzNHYsWP19NNPa+LEiVqzZo127typl156SZIUFxenhx56SI8//rj69eunrKwsLV68WG63W/n5+ZKkgQMHavz48Zo5c6aKiorU2Nio2bNna9KkSdYMv9WrV6tLly7Kzs5WYmKidu7cqUWLFqmgoEBdunRpj74KmZZb0LTM9LvnxXeZ6QcAQBgJOlAVFBTos88+05IlS1RZWamcnBwVFxdbg8qPHj0qm+3cB/2YMWP0yiuv6NFHH9Ujjzyifv36af369RoyZIhV8/DDD6u+vl6zZs1STU2NbrvtNhUXF8vhOPdNzOrVqzV79myNGzdONptNd999t1asWHHujdjteuKJJ/TXv/5Vxhj16dNHs2fP1r/9279dU8eEm9Zm+nHZDwCA8BD0OlTRLBzWsbiUhrNNuuU/3tDpxmYldYnX7sVeORKCzsMAAESdcPj85ppRhOAWNAAAhC8CVYRgYDoAAOGLQBUhWgam93d1twamNzVH9uxFAACiBYEqgnALGgAAwhOBKoJwCxoAAMITgSrScAsaAADCDoEqgrR2CxoAABB6BKoIwiU/AADCE4Eq0nDJDwCAsEOgiiAXXvI7crI+xC0CAAASgSqinH/JT5Lmvvoea1EBABAGCFQRxB5v0y8LcqznDEwHACA8EKgiTFZaNwamAwAQZghUkYiB6QAAhBUCVYRhYDoAAOGHQBVhGJgOAED4IVBFGAamAwAQfghUEYiB6QAAhBcCVaRiYDoAAGGDQBWBuEkyAADhhUAVgXqndtXQXk5JUn9Xd7mdjhC3CACA2EagikD2eJte/dFo9Xd1V0VVne558V1m+gEAEEIEqgh13Negiqo6SdJeLvsBABBSBKoI5XY6lNQlXpKU1CWey34AAIQQgSpCHfc16HRjsyTpdGOzjvsaQtwiAABiF4EqQjEwHQCA8EGgilAMTAcAIHwQqCIYA9MBAAgPBKoIxsB0AADCA4EqgjEwHQCA8ECgimAMTAcAIDwQqCIYA9MBAAgPBKoIx8B0AABCj0AV4XqndlW2O1mSlN0rWb1Tu4a4RQAAxB4CVTSIi2v5Q0ibAQBArCJQRbij1V9q3yc+SdI+LvkBABASBKoIx1pUAACEHoEqwrEWFQAAoUeginCsRQUAQOgRqCIca1EBABB6BKoowFpUAACEFoEqCjAwHQCA0CJQRYELB6Yf++J0iFsEAEBsIVBFgfNXS5ekua++xzgqAAA6EYEqCtjjbfplQY71nAU+AQDoXASqKJGV1o17+gEAECIEqmjCPf0AAAgJAlWUuPCefkdO1oe4RQAAxA4CVZRgYDoAAKFDoIoSDEwHACB0CFRRhIHpAACEBoEq2jAwHQCATkegiiIMTAcAIDQIVFGEgekAAIQGgSqKMDAdAIDQIFBFGQamAwDQ+QhU0YiB6QAAdCoCVZRhYDoAAJ2PQBVlGJgOAEDnI1BFGQamAwDQ+QhUUSjzuiQldYmXJCV1iZfb6QhxiwAAiG4Eqih03Neg043NkqTTjc067msIcYsAAIhuBKoodP44KpZOAACg4xGoohVLJwAA0GmuKVCtXLlSffv2lcPhkMfj0fbt2y9bv27dOg0YMEAOh0PZ2dnauHFjwH5jjJYsWaKMjAwlJSXJ6/Xq0KFDATXV1dWaMmWKkpOTlZKSohkzZqiurs7av2XLFt11113KyMhQt27dlJOTo9WrV1/L24t4LJ0AAEDnCjpQrV27VnPnztXSpUu1a9cuDRs2THl5eTpx4kSr9du2bdPkyZM1Y8YM7d69W/n5+crPz9f+/futmuXLl2vFihUqKipSWVmZunXrpry8PDU0nBv7M2XKFL3//vvatGmTNmzYoK1bt2rWrFkBrzN06FD94Q9/0N69ezV9+nRNnTpVGzZsCPYtRjyWTgAAoHPFGWNMMAd4PB6NHDlSzz//vCTJ7/crMzNTDz74oBYuXHhRfUFBgerr6wOCzejRo5WTk6OioiIZY+R2uzVv3jzNnz9fkuTz+eRyubRq1SpNmjRJBw8e1KBBg7Rjxw6NGDFCklRcXKwJEybo448/ltvtbrWtEydOlMvl0ssvv3xV7622tlZOp1M+n0/JyclXPiCMHao6pX95Zqv1fPO8sbrx+u4hbBEAAB0jHD6/g/qG6uzZsyovL5fX6z33C2w2eb1elZaWtnpMaWlpQL0k5eXlWfVHjhxRZWVlQI3T6ZTH47FqSktLlZKSYoUpSfJ6vbLZbCorK7tke30+n1JTUy+5/8yZM6qtrQ14RAuWTgAAoPMEFahOnjyp5uZmuVyugO0ul0uVlZWtHlNZWXnZ+pafV6rp2bNnwH673a7U1NRLvu6rr76qHTt2aPr06Zd8P4WFhXI6ndYjMzPzkrWR5sKlE459cTrELQIAIHpF5Sy/N998U9OnT9evf/1rDR48+JJ1ixYtks/nsx7Hjh3rxFZ2LMZRAQDQeYIKVGlpaYqPj1dVVVXA9qqqKqWnp7d6THp6+mXrW35eqebCQe9NTU2qrq6+6HX/8pe/6Dvf+Y6eeeYZTZ069bLvJzExUcnJyQGPaMEtaAAA6DxBBaqEhAQNHz5cJSUl1ja/36+SkhLl5ua2ekxubm5AvSRt2rTJqs/KylJ6enpATW1trcrKyqya3Nxc1dTUqLy83KrZvHmz/H6/PB6PtW3Lli2aOHGinnjiiYAZgLEqK60bC3wCANAJ7MEeMHfuXE2bNk0jRozQqFGj9Oyzz6q+vt4aqzR16lT16tVLhYWFkqQ5c+Zo7NixevrppzVx4kStWbNGO3fu1EsvvSRJiouL00MPPaTHH39c/fr1U1ZWlhYvXiy32638/HxJ0sCBAzV+/HjNnDlTRUVFamxs1OzZszVp0iRrht+bb76pb3/725ozZ47uvvtua2xVQkLCZQemRz0W+AQAoOOZa/Dcc8+Z3r17m4SEBDNq1Cjz7rvvWvvGjh1rpk2bFlD/6quvmptvvtkkJCSYwYMHm9dffz1gv9/vN4sXLzYul8skJiaacePGmYqKioCazz//3EyePNl0797dJCcnm+nTp5tTp05Z+6dNm2YkXfQYO3bsVb8vn89nJBmfz3f1nRHGPjxxyvRZsMF6/LWyNtRNAgCg3YXD53fQ61BFs3BYx6I9NTX79b2V72jf8a+Wg8ju5dQffzJG9vionIsAAIhR4fD5zSdrFGNgOgAAnYNAFeVY4BMAgI5HoIpyLPAJAEDHI1BFORb4BACg4xGoohzjqAAA6HgEqhjAAp8AAHQsAlWsYIFPAAA6DIEqBhyt/lL7PvFJ4pIfAAAdgUAVA9xOB0snAADQgQhUMYClEwAA6FgEqhjA0gkAAHQsAlUMYOkEAAA6FoEqRnALGgAAOg6BKkYwjgoAgI5DoIoRjKMCAKDjEKhiBOOoAADoOASqGMI4KgAAOgaBKoYwjgoAgI5BoIohjKMCAKBjEKhiCOOoAADoGASqGJOV1s36liq7V7J6p3YNcYsAAIh8BKpYFBfX8oeQNgMAgGhBoIoxR6u/1L5PfJK+uuR35GR9iFsEAEDkI1DFGAamAwDQ/ghUMYaB6QAAtD8CVQxigU8AANoXgSoGscAnAADti0AVgxhHBQBA+yJQxSDGUQEA0L4IVDGKcVQAALQfAlWMunAc1XFfQ4hbBABA5CJQxSi308E3VAAAtBMCVYxiph8AAO2HQBWjmOkHAED7IVDFKGb6AQDQfghUMYyZfgAAtA8CVQxjHBUAAO2DQBXDGEcFAED7IFDFMMZRAQDQPghUMe78cVSJdpt6dk8IcYsAAIg8BKoYd/44qjNNft1dVMplPwAAgkSginG9U7uqf8/u1vOKqjou+wEAECQCVYyzx9v0h/tzlWj/6lRg+QQAAIJHoIJO1J3VmaavLvOxfAIAAMEjUIHlEwAAaCMCFVg+AQCANiJQQRK3oQEAoC0IVJB08W1ojvsaQtwiAAAiB4EKkiS308E3VAAAXCMCFSRxo2QAANqCQAVJzPQDAKAtCFSQxEw/AADagkAFCzdKBgDg2hCoYOFGyQAAXBsCFSzcKBkAgGtDoIKFGyUDAHBtCFQIwI2SAQAIHoEKAVg+AQCA4BGoEIDlEwAACB6BChdh+QQAAIJDoMJFWD4BAIDgEKhwEZZPAAAgOAQqXITlEwAACA6BCq1i+QQAAK4egQqtunD5hH9bu4dxVAAAXMI1BaqVK1eqb9++cjgc8ng82r59+2Xr161bpwEDBsjhcCg7O1sbN24M2G+M0ZIlS5SRkaGkpCR5vV4dOnQooKa6ulpTpkxRcnKyUlJSNGPGDNXV1Vn7GxoadN999yk7O1t2u135+fnX8tbwD/Z4m578P8Os5/uP1+rIyfoQtggAgPAVdKBau3at5s6dq6VLl2rXrl0aNmyY8vLydOLEiVbrt23bpsmTJ2vGjBnavXu38vPzlZ+fr/3791s1y5cv14oVK1RUVKSysjJ169ZNeXl5amhosGqmTJmi999/X5s2bdKGDRu0detWzZo1y9rf3NyspKQk/fSnP5XX6w32baEV8ba4UDcBAICIEGeMMcEc4PF4NHLkSD3//POSJL/fr8zMTD344INauHDhRfUFBQWqr6/Xhg0brG2jR49WTk6OioqKZIyR2+3WvHnzNH/+fEmSz+eTy+XSqlWrNGnSJB08eFCDBg3Sjh07NGLECElScXGxJkyYoI8//lhutzvgNe+77z7V1NRo/fr1QXVGbW2tnE6nfD6fkpOTr3xAlGs426Rb/uMNnW5sVlKXeO1e7JUjwR7qZgEAECAcPr+D+obq7NmzKi8vD/gGyGazyev1qrS0tNVjSktLL/rGKC8vz6o/cuSIKisrA2qcTqc8Ho9VU1paqpSUFCtMSZLX65XNZlNZWVkwbyHAmTNnVFtbG/DAOeevR8XAdAAALi2oQHXy5Ek1NzfL5XIFbHe5XKqsrGz1mMrKysvWt/y8Uk3Pnj0D9tvtdqWmpl7yda9GYWGhnE6n9cjMzLzm3xWNGJgOAMDVielZfosWLZLP57Mex44dC3WTwgoD0wEAuDpBBaq0tDTFx8erqqoqYHtVVZXS09NbPSY9Pf2y9S0/r1Rz4aD3pqYmVVdXX/J1r0ZiYqKSk5MDHgh04cD0Zn9QQ+4AAIgJQQWqhIQEDR8+XCUlJdY2v9+vkpIS5ebmtnpMbm5uQL0kbdq0yarPyspSenp6QE1tba3KysqsmtzcXNXU1Ki8vNyq2bx5s/x+vzweTzBvAUHKSuumIedd9pu/7j0u+wEAcIGgL/nNnTtXv/71r/Xb3/5WBw8e1P3336/6+npNnz5dkjR16lQtWrTIqp8zZ46Ki4v19NNP64MPPtAvfvEL7dy5U7Nnz5YkxcXF6aGHHtLjjz+uP//5z9q3b5+mTp0qt9ttrSU1cOBAjR8/XjNnztT27dv1zjvvaPbs2Zo0aVLADL8DBw5oz549qq6uls/n0549e7Rnz542dA/s8TY9xWU/AAAuK+g58AUFBfrss8+0ZMkSVVZWKicnR8XFxdag8qNHj8pmO5fTxowZo1deeUWPPvqoHnnkEfXr10/r16/XkCFDrJqHH35Y9fX1mjVrlmpqanTbbbepuLhYDse5+8etXr1as2fP1rhx42Sz2XT33XdrxYoVAW2bMGGCPvroI+v5LbfcIumrhUNx7bjsBwDA5QW9DlU0C4d1LMJRU7Nf+Svf0f7jXy0rkd3LqT/+ZIzs8TE9pwEAECbC4fObT0RckT3epmcKcqzn+z7x6Wj1l6FrEAAAYYZAhauSeV2SkrrES5IS7Tb17J4Q4hYBABA+CFS4Kuevmn6mya+7i0qZ7QcAwD8QqHBVeqd2Vf+e3a3nFVV1XPYDAOAfCFS4KvZ4m/5wf64S7V+dMlz2AwDgHAIVrtqJurM60/TVZT4u+wEAcA6BCleNy34AALSOQIWr1nLZLyH+q4U+HXab3E7HFY4CACD6EagQlE9rz+hs81drwTY0+XXsi9MhbhEAAKFHoEKbcBsaAAAIVAhSVlo3DXGfW9Z//rr3GJgOAIh5BCoExR5v01P/Z5j1fP/xWh0+URfCFgEAEHoEKgQt3hYX8Pyna3bzLRUAIKYRqBC0rLRuuvm85RP+yvIJAIAYR6BC0OzxNr3GqukAAFgIVLgmF66a/v0XtnHZDwAQswhUuCa9U7vq5uu7Wc//eqJeR07Wh7BFAACEDoEK18Qeb9P/nXxLwDbWpAIAxCoCFa5ZP1cP1qQCAEAEKrRBa2tScdkPABCLCFRoV1z2AwDEIgIV2uTCRT4BAIhFBCq0Cff2AwCAQIU24t5+AAAQqNAOLrzs9+Dvd/EtFQAgphCo0GZZad3U77xFPg+xyCcAIMYQqNBm9nibVrDIJwAghhGo0C5Y5BMAEMsIVGgXDE4HAMQyAhXaDYPTAQCxikCFdtPa4HS+pQIAxAICFdpNa4PT+ZYKABALCFRoV/1cPVhCAQAQcwhUaFetfUsFAEC0I1Ch3WWldVNSl3hJUqLdpozkxBC3CACAjkWgQrs77mvQ6cZmSdKZJr++/8I2xlEBAKIagQrtrndqV9183jiqvzLbDwAQ5QhUaHf2eJv+L7P9AAAxhECFDtHabD++pQIARCsCFToEa1IBAGIJgQodhm+pAACxgkCFDsO3VACAWEGgQofiWyoAQCwgUKFDtfYt1exXyvmWCgAQVQhU6HD9XD10U1pX6/nhz75UReWpELYIAID2RaBCh7PH27TgzoEB236ymm+pAADRg0CFTnF7vzQlxMdZzz+qPs1YKgBA1CBQoVM4Eux67f7cgG3M+AMARAsCFTrNgAwnM/4AAFGJQIVO09qMvx/9v51qONsUohYBANA+CFToVBeuS/VR9Wl95/m3ufQHAIhoBCp0qta+pTp0ol5HTtaHqEUAALQdgQqdrp+rh266vmvAttNnm0PUGgAA2o5AhU5nj7fpucm3Bmy7u2ib6k6fDVGLAABoGwIVQqKfq4f6pp77lqqx2Sjv2a0MUAcARCQCFULCHm/Thgf/SfbzzsBPfGcIVQCAiESgQsh0T0rQ+p+MCdjGrD8AQCQiUCGkBmQ41a9nt4Bth07Uc/NkAEBEIVAhpOzxNv1/s29Tn9SkgO35K99mkDoAIGIQqBByjgS7/uehO+R2JlrbGv3StxhPBQCIEAQqhAVHgl0v3Ts8YNtxBqkDACIEgQphY0CGUzddHzie6qPq0/rm01u4/AcACGsEKoSNr5ZSuHg81XHfGeX8xybV1DeEqGUAAFwegQphpWU81YWhqskv3fofJXrvaDVLKgAAws41BaqVK1eqb9++cjgc8ng82r59+2Xr161bpwEDBsjhcCg7O1sbN24M2G+M0ZIlS5SRkaGkpCR5vV4dOnQooKa6ulpTpkxRcnKyUlJSNGPGDNXV1QXU7N27V7fffrscDocyMzO1fPnya3l7CLGWUHXh/f78ku76Vam+8eSb2v/xFwQrAEDYiDPGmGAOWLt2raZOnaqioiJ5PB49++yzWrdunSoqKtSzZ8+L6rdt26Y77rhDhYWF+va3v61XXnlFTzzxhHbt2qUhQ4ZIkp544gkVFhbqt7/9rbKysrR48WLt27dPBw4ckMPhkCTdeeed+vTTT/Xiiy+qsbFR06dP18iRI/XKK69Ikmpra3XzzTfL6/Vq0aJF2rdvn374wx/q2Wef1axZs67qvdXW1srpdMrn8yk5OTmYbkEHaGr2a9OBKt2/eler+3s5E7Rw/EDZbDY1+40+qzujnt0TWn0u6aq2RdNx4dgm3gt9EK5tog865734Ghr1vRy3uiclqD2Fw+d30IHK4/Fo5MiRev755yVJfr9fmZmZevDBB7Vw4cKL6gsKClRfX68NGzZY20aPHq2cnBwVFRXJGCO326158+Zp/vz5kiSfzyeXy6VVq1Zp0qRJOnjwoAYNGqQdO3ZoxIgRkqTi4mJNmDBBH3/8sdxut1544QX9/Oc/V2VlpRISvvoPtXDhQq1fv14ffPDBVb23cPgPgkBNzX5997m3dKCy7srFAICwFydp39J/addQFQ6f30Fd8jt79qzKy8vl9XrP/QKbTV6vV6Wlpa0eU1paGlAvSXl5eVb9kSNHVFlZGVDjdDrl8XismtLSUqWkpFhhSpK8Xq9sNpvKysqsmjvuuMMKUy2vU1FRoS+++KLVtp05c0a1tbUBD4QXe7xNf37wdr3+4G3qfZ0j1M0BALSRkfT6vspQN6PdBRWoTp48qebmZrlcroDtLpdLlZWtd05lZeVl61t+XqnmwsuJdrtdqampATWt/Y7zX+NChYWFcjqd1iMzM7P1N46QssfbNLiXU5vnf0N/fuCfZI8LdYsAANcqTtLE7PRQN6Pd2UPdgFBatGiR5s6daz2vra0lVIUxe7xNQzNTtP/f8/TWoZM623RuUHokjDPo7OPCsU28F/ogXNtEH0T2GKpwEFSgSktLU3x8vKqqqgK2V1VVKT299bSZnp5+2fqWn1VVVcrIyAioycnJsWpOnDgR8DuamppUXV0d8Htae53zX+NCiYmJSkxMbHUfwpcjwa5/GRx9/3cDAIhcQV3yS0hI0PDhw1VSUmJt8/v9KikpUW5ubqvH5ObmBtRL0qZNm6z6rKwspaenB9TU1taqrKzMqsnNzVVNTY3Ky8utms2bN8vv98vj8Vg1W7duVWNjY8Dr9O/fX9ddd10wbxMAACA4Jkhr1qwxiYmJZtWqVebAgQNm1qxZJiUlxVRWVhpjjLn33nvNwoULrfp33nnH2O1289RTT5mDBw+apUuXmi5duph9+/ZZNcuWLTMpKSnmT3/6k9m7d6+56667TFZWljl9+rRVM378eHPLLbeYsrIy8/bbb5t+/fqZyZMnW/tramqMy+Uy9957r9m/f79Zs2aN6dq1q3nxxRev+r35fD4jyfh8vmC7BQAAhEg4fH4HHaiMMea5554zvXv3NgkJCWbUqFHm3XfftfaNHTvWTJs2LaD+1VdfNTfffLNJSEgwgwcPNq+//nrAfr/fbxYvXmxcLpdJTEw048aNMxUVFQE1n3/+uZk8ebLp3r27SU5ONtOnTzenTp0KqHnvvffMbbfdZhITE02vXr3MsmXLgnpf4fAfBAAABCccPr+DXocqmoXDOhYAACA44fD5zb38AAAA2ohABQAA0EYEKgAAgDYiUAEAALQRgQoAAKCNCFQAAABtRKACAABoIwIVAABAGwV1c+Ro17LGaW1tbYhbAgAArlbL53Yo1yonUJ3n1KlTkqTMzMwQtwQAAATr1KlTcjqdIXltbj1zHr/fr+PHj6tHjx6Ki4tr199dW1urzMxMHTt2LOZva0NfnENfnENfnENfnENfBKI/zjm/L3r06KFTp07J7XbLZgvNaCa+oTqPzWbTDTfc0KGvkZycHPN/CVrQF+fQF+fQF+fQF+fQF4Hoj3Na+iJU30y1YFA6AABAGxGoAAAA2ohA1UkSExO1dOlSJSYmhropIUdfnENfnENfnENfnENfBKI/zgm3vmBQOgAAQBvxDRUAAEAbEagAAADaiEAFAADQRgQqAACANiJQdYKVK1eqb9++cjgc8ng82r59e6ibFJTCwkKNHDlSPXr0UM+ePZWfn6+KioqAmn/+539WXFxcwOPHP/5xQM3Ro0c1ceJEde3aVT179tTPfvYzNTU1BdRs2bJFt956qxITE3XTTTdp1apVF7UnlP35i1/84qL3OWDAAGt/Q0ODHnjgAX3ta19T9+7ddffdd6uqqirgd0RDP0hS3759L+qLuLg4PfDAA5Ki+5zYunWrvvOd78jtdisuLk7r168P2G+M0ZIlS5SRkaGkpCR5vV4dOnQooKa6ulpTpkxRcnKyUlJSNGPGDNXV1QXU7N27V7fffrscDocyMzO1fPnyi9qybt06DRgwQA6HQ9nZ2dq4cWPQbWmry/VHY2OjFixYoOzsbHXr1k1ut1tTp07V8ePHA35Ha+fTsmXLAmoioT+udG7cd999F73P8ePHB9REy7lxpb5o7d+PuLg4Pfnkk1ZNRJ0XBh1qzZo1JiEhwbz88svm/fffNzNnzjQpKSmmqqoq1E27anl5eeY3v/mN2b9/v9mzZ4+ZMGGC6d27t6mrq7Nqxo4da2bOnGk+/fRT6+Hz+az9TU1NZsiQIcbr9Zrdu3ebjRs3mrS0NLNo0SKr5m9/+5vp2rWrmTt3rjlw4IB57rnnTHx8vCkuLrZqQt2fS5cuNYMHDw54n5999pm1/8c//rHJzMw0JSUlZufOnWb06NFmzJgxUdcPxhhz4sSJgH7YtGmTkWTefPNNY0x0nxMbN240P//5z81rr71mJJk//vGPAfuXLVtmnE6nWb9+vXnvvffMd7/7XZOVlWVOnz5t1YwfP94MGzbMvPvuu+att94yN910k5k8ebK13+fzGZfLZaZMmWL2799vfv/735ukpCTz4osvWjXvvPOOiY+PN8uXLzcHDhwwjz76qOnSpYvZt29fUG3pyP6oqakxXq/XrF271nzwwQemtLTUjBo1ygwfPjzgd/Tp08c89thjAefL+f/GREp/XOncmDZtmhk/fnzA+6yurg6oiZZz40p9cX4ffPrpp+bll182cXFx5sMPP7RqIum8IFB1sFGjRpkHHnjAet7c3GzcbrcpLCwMYava5sSJE0aS+ctf/mJtGzt2rJkzZ84lj9m4caOx2WymsrLS2vbCCy+Y5ORkc+bMGWOMMQ8//LAZPHhwwHEFBQUmLy/Peh7q/ly6dKkZNmxYq/tqampMly5dzLp166xtBw8eNJJMaWmpMSZ6+qE1c+bMMV//+teN3+83xsTOOXHhB4Xf7zfp6enmySeftLbV1NSYxMRE8/vf/94YY8yBAweMJLNjxw6r5r//+79NXFyc+eSTT4wxxvzqV78y1113ndUXxhizYMEC079/f+v5PffcYyZOnBjQHo/HY370ox9ddVvaW2sfnBfavn27kWQ++ugja1ufPn3MM888c8ljIrE/LhWo7rrrrkseE63nxtWcF3fddZf55je/GbAtks4LLvl1oLNnz6q8vFxer9faZrPZ5PV6VVpaGsKWtY3P55MkpaamBmxfvXq10tLSNGTIEC1atEhffvmlta+0tFTZ2dlyuVzWtry8PNXW1ur999+3as7vq5aalr4Kl/48dOiQ3G63brzxRk2ZMkVHjx6VJJWXl6uxsTGgfQMGDFDv3r2t9kVTP5zv7Nmz+t3vfqcf/vCHATcWj5Vz4nxHjhxRZWVlQJucTqc8Hk/AeZCSkqIRI0ZYNV6vVzabTWVlZVbNHXfcoYSEBKsmLy9PFRUV+uKLL6yay/XP1bQlFHw+n+Li4pSSkhKwfdmyZfra176mW265RU8++WTA5d9o6o8tW7aoZ8+e6t+/v+6//359/vnn1r5YPTeqqqr0+uuva8aMGRfti5Tzgpsjd6CTJ0+qubk54ANDklwulz744IMQtapt/H6/HnroIf3TP/2ThgwZYm3/wQ9+oD59+sjtdmvv3r1asGCBKioq9Nprr0mSKisrW+2Hln2Xq6mtrdXp06f1xRdfhLw/PR6PVq1apf79++vTTz/Vv//7v+v222/X/v37VVlZqYSEhIs+JFwu1xXfY8u+y9WEUz9caP369aqpqdF9991nbYuVc+JCLW1vrU3nv6+ePXsG7Lfb7UpNTQ2oycrKuuh3tOy77rrrLtk/5/+OK7WlszU0NGjBggWaPHlywM19f/rTn+rWW29Vamqqtm3bpkWLFunTTz/VL3/5S0nR0x/jx4/X97//fWVlZenDDz/UI488ojvvvFOlpaWKj4+P2XPjt7/9rXr06KHvf//7Adsj6bwgUCEoDzzwgPbv36+33347YPusWbOsP2dnZysjI0Pjxo3Thx9+qK9//eud3cwOc+edd1p/Hjp0qDwej/r06aNXX31VSUlJIWxZaP3Xf/2X7rzzTrndbmtbrJwTuHqNjY265557ZIzRCy+8ELBv7ty51p+HDh2qhIQE/ehHP1JhYWHY3FqkPUyaNMn6c3Z2toYOHaqvf/3r2rJli8aNGxfCloXWyy+/rClTpsjhcARsj6Tzgkt+HSgtLU3x8fEXzfKqqqpSenp6iFp17WbPnq0NGzbozTff1A033HDZWo/HI0k6fPiwJCk9Pb3VfmjZd7ma5ORkJSUlhWV/pqSk6Oabb9bhw4eVnp6us2fPqqam5pLti8Z++Oijj/TGG2/oX//1Xy9bFyvnRMvrXq5N6enpOnHiRMD+pqYmVVdXt8u5cv7+K7Wls7SEqY8++kibNm0K+HaqNR6PR01NTfr73/8uKfr6o8WNN96otLS0gL8XsXZuvPXWW6qoqLjivyFSeJ8XBKoOlJCQoOHDh6ukpMTa5vf7VVJSotzc3BC2LDjGGM2ePVt//OMftXnz5ou+Xm3Nnj17JEkZGRmSpNzcXO3bty/gH4qWf1QHDRpk1ZzfVy01LX0Vjv1ZV1enDz/8UBkZGRo+fLi6dOkS0L6KigodPXrUal809sNvfvMb9ezZUxMnTrxsXaycE1lZWUpPTw9oU21trcrKygLOg5qaGpWXl1s1mzdvlt/vt4Jnbm6utm7dqsbGRqtm06ZN6t+/v6677jqr5nL9czVt6QwtYerQoUN644039LWvfe2Kx+zZs0c2m826/BVN/XG+jz/+WJ9//nnA34tYOjekr77hHj58uIYNG3bF2rA+L656+DquyZo1a0xiYqJZtWqVOXDggJk1a5ZJSUkJmNkU7u6//37jdDrNli1bAqaufvnll8YYYw4fPmwee+wxs3PnTnPkyBHzpz/9ydx4443mjjvusH5HyxT5b33rW2bPnj2muLjYXH/99a1Okf/Zz35mDh48aFauXNnqFPlQ9ue8efPMli1bzJEjR8w777xjvF6vSUtLMydOnDDGfLVsQu/evc3mzZvNzp07TW5ursnNzY26fmjR3NxsevfubRYsWBCwPdrPiVOnTpndu3eb3bt3G0nml7/8pdm9e7c1a23ZsmUmJSXF/OlPfzJ79+41d911V6vLJtxyyy2mrKzMvP3226Zfv34BU+NramqMy+Uy9957r9m/f79Zs2aN6dq160XTwe12u3nqqafMwYMHzdKlS1udDn6ltnRkf5w9e9Z897vfNTfccIPZs2dPwL8hLTOztm3bZp555hmzZ88e8+GHH5rf/e535vrrrzdTp06NuP64XF+cOnXKzJ8/35SWlpojR46YN954w9x6662mX79+pqGhwfod0XJuXOnviTFfLXvQtWtX88ILL1x0fKSdFwSqTvDcc8+Z3r17m4SEBDNq1Cjz7rvvhrpJQZHU6uM3v/mNMcaYo0ePmjvuuMOkpqaaxMREc9NNN5mf/exnAWsOGWPM3//+d3PnnXeapKQkk5aWZubNm2caGxsDat58802Tk5NjEhISzI033mi9xvlC2Z8FBQUmIyPDJCQkmF69epmCggJz+PBha//p06fNT37yE3PdddeZrl27mu9973vm008/Dfgd0dAPLf7nf/7HSDIVFRUB26P9nHjzzTdb/Tsxbdo0Y8xX07AXL15sXC6XSUxMNOPGjbuojz7//HMzefJk0717d5OcnGymT59uTp06FVDz3nvvmdtuu80kJiaaXr16mWXLll3UlldffdXcfPPNJiEhwQwePNi8/vrrAfuvpi0d2R9Hjhy55L8hLWuWlZeXG4/HY5xOp3E4HGbgwIHmP//zPwNCRqT0x+X64ssvvzTf+ta3zPXXX2+6dOli+vTpY2bOnHlR+I+Wc+NKf0+MMebFF180SUlJpqam5qLjI+28iDPGmKv/PgsAAAAXYgwVAABAGxGoAAAA2ohABQAA0EYEKgAAgDYiUAEAALQRgQoAAKCNCFQAAABtRKACAABoIwIVAABAGxGoAAAA2ohABQAA0EYEKgAAgDb6/wEgi6nvNDHnLAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------fold0---------\n",
            "train:53791 valid:13417\n",
            "\n",
            "resume from epoch322\n",
            "load weights from ./aslfr-fp16-192d-17l-ctcattjoint-seed42-fold0-last.h5\n",
            "Epoch 1/78\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py:1514: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py:1497: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  6/420 [..............................] - ETA: 1:55 - loss: 31.3672 - att_decoder_loss: 37.7232 - ctc_decoder_loss: 12.2993 - att_decoder_acc: 0.7853"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0105s vs `on_train_batch_end` time: 35.1484s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "420/420 [==============================] - 684s 368ms/step - loss: 31.3537 - att_decoder_loss: 37.9491 - ctc_decoder_loss: 11.5678 - att_decoder_acc: 0.7970 - val_loss: 31.8597 - val_att_decoder_loss: 37.4433 - val_ctc_decoder_loss: 15.1088 - val_att_decoder_acc: 0.8228\n",
            "Epoch 2/78\n",
            "420/420 [==============================] - 143s 339ms/step - loss: 31.3492 - att_decoder_loss: 37.9398 - ctc_decoder_loss: 11.5773 - att_decoder_acc: 0.7970 - val_loss: 31.9287 - val_att_decoder_loss: 37.4770 - val_ctc_decoder_loss: 15.2838 - val_att_decoder_acc: 0.8219\n",
            "Epoch 3/78\n",
            "420/420 [==============================] - 145s 345ms/step - loss: 31.3770 - att_decoder_loss: 37.9837 - ctc_decoder_loss: 11.5570 - att_decoder_acc: 0.7973 - val_loss: 31.8481 - val_att_decoder_loss: 37.4387 - val_ctc_decoder_loss: 15.0763 - val_att_decoder_acc: 0.8230\n",
            "Epoch 4/78\n",
            "420/420 [==============================] - 144s 342ms/step - loss: 31.3292 - att_decoder_loss: 37.9282 - ctc_decoder_loss: 11.5323 - att_decoder_acc: 0.7978 - val_loss: 31.9127 - val_att_decoder_loss: 37.4825 - val_ctc_decoder_loss: 15.2032 - val_att_decoder_acc: 0.8221\n",
            "Epoch 5/78\n",
            "420/420 [==============================] - 143s 339ms/step - loss: 31.2786 - att_decoder_loss: 37.8823 - ctc_decoder_loss: 11.4678 - att_decoder_acc: 0.7985 - val_loss: 31.9134 - val_att_decoder_loss: 37.4994 - val_ctc_decoder_loss: 15.1553 - val_att_decoder_acc: 0.8220\n",
            "Epoch 6/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 31.3274 - att_decoder_loss: 37.9364 - ctc_decoder_loss: 11.5007 - att_decoder_acc: 0.7978 - val_loss: 31.8689 - val_att_decoder_loss: 37.4485 - val_ctc_decoder_loss: 15.1304 - val_att_decoder_acc: 0.8227\n",
            "Epoch 7/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 31.2834 - att_decoder_loss: 37.8675 - ctc_decoder_loss: 11.5312 - att_decoder_acc: 0.7989 - val_loss: 31.8594 - val_att_decoder_loss: 37.4483 - val_ctc_decoder_loss: 15.0927 - val_att_decoder_acc: 0.8225\n",
            "Epoch 8/78\n",
            "420/420 [==============================] - 143s 339ms/step - loss: 31.3070 - att_decoder_loss: 37.9063 - ctc_decoder_loss: 11.5088 - att_decoder_acc: 0.7980 - val_loss: 31.8590 - val_att_decoder_loss: 37.4445 - val_ctc_decoder_loss: 15.1027 - val_att_decoder_acc: 0.8226\n",
            "Epoch 9/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 31.2141 - att_decoder_loss: 37.8260 - ctc_decoder_loss: 11.3785 - att_decoder_acc: 0.7998 - val_loss: 31.8599 - val_att_decoder_loss: 37.4368 - val_ctc_decoder_loss: 15.1292 - val_att_decoder_acc: 0.8230\n",
            "Epoch 10/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 31.2398 - att_decoder_loss: 37.8467 - ctc_decoder_loss: 11.4191 - att_decoder_acc: 0.7995 - val_loss: 31.8831 - val_att_decoder_loss: 37.4496 - val_ctc_decoder_loss: 15.1835 - val_att_decoder_acc: 0.8228\n",
            "Epoch 11/78\n",
            "420/420 [==============================] - 144s 343ms/step - loss: 31.1917 - att_decoder_loss: 37.8091 - ctc_decoder_loss: 11.3395 - att_decoder_acc: 0.8006 - val_loss: 31.8545 - val_att_decoder_loss: 37.4518 - val_ctc_decoder_loss: 15.0627 - val_att_decoder_acc: 0.8227\n",
            "Epoch 12/78\n",
            "420/420 [==============================] - 144s 342ms/step - loss: 31.2238 - att_decoder_loss: 37.8282 - ctc_decoder_loss: 11.4107 - att_decoder_acc: 0.7999 - val_loss: 31.8888 - val_att_decoder_loss: 37.4515 - val_ctc_decoder_loss: 15.2009 - val_att_decoder_acc: 0.8225\n",
            "Epoch 13/78\n",
            "420/420 [==============================] - 146s 348ms/step - loss: 31.2811 - att_decoder_loss: 37.8793 - ctc_decoder_loss: 11.4867 - att_decoder_acc: 0.7989 - val_loss: 31.8440 - val_att_decoder_loss: 37.4294 - val_ctc_decoder_loss: 15.0880 - val_att_decoder_acc: 0.8227\n",
            "Epoch 14/78\n",
            "420/420 [==============================] - 146s 347ms/step - loss: 31.1644 - att_decoder_loss: 37.7733 - ctc_decoder_loss: 11.3376 - att_decoder_acc: 0.8007 - val_loss: 31.8594 - val_att_decoder_loss: 37.4431 - val_ctc_decoder_loss: 15.1084 - val_att_decoder_acc: 0.8229\n",
            "Epoch 15/78\n",
            "420/420 [==============================] - 144s 342ms/step - loss: 31.1401 - att_decoder_loss: 37.7569 - ctc_decoder_loss: 11.2897 - att_decoder_acc: 0.8015 - val_loss: 31.8813 - val_att_decoder_loss: 37.4721 - val_ctc_decoder_loss: 15.1089 - val_att_decoder_acc: 0.8222\n",
            "Epoch 16/78\n",
            "420/420 [==============================] - 144s 343ms/step - loss: 31.1709 - att_decoder_loss: 37.7866 - ctc_decoder_loss: 11.3238 - att_decoder_acc: 0.8010 - val_loss: 31.8709 - val_att_decoder_loss: 37.4643 - val_ctc_decoder_loss: 15.0904 - val_att_decoder_acc: 0.8225\n",
            "Epoch 17/78\n",
            "420/420 [==============================] - 145s 346ms/step - loss: 31.1641 - att_decoder_loss: 37.7853 - ctc_decoder_loss: 11.3006 - att_decoder_acc: 0.8009 - val_loss: 31.8690 - val_att_decoder_loss: 37.4477 - val_ctc_decoder_loss: 15.1329 - val_att_decoder_acc: 0.8228\n",
            "Epoch 18/78\n",
            "420/420 [==============================] - 144s 344ms/step - loss: 31.1754 - att_decoder_loss: 37.7871 - ctc_decoder_loss: 11.3403 - att_decoder_acc: 0.8012 - val_loss: 31.8641 - val_att_decoder_loss: 37.4339 - val_ctc_decoder_loss: 15.1546 - val_att_decoder_acc: 0.8229\n",
            "Epoch 19/78\n",
            "420/420 [==============================] - 144s 343ms/step - loss: 31.1757 - att_decoder_loss: 37.7866 - ctc_decoder_loss: 11.3430 - att_decoder_acc: 0.8012 - val_loss: 31.9101 - val_att_decoder_loss: 37.4723 - val_ctc_decoder_loss: 15.2235 - val_att_decoder_acc: 0.8224\n",
            "Epoch 20/78\n",
            "420/420 [==============================] - 145s 344ms/step - loss: 31.0867 - att_decoder_loss: 37.7055 - ctc_decoder_loss: 11.2303 - att_decoder_acc: 0.8027 - val_loss: 31.8526 - val_att_decoder_loss: 37.4270 - val_ctc_decoder_loss: 15.1293 - val_att_decoder_acc: 0.8230\n",
            "Epoch 21/78\n",
            "420/420 [==============================] - 145s 346ms/step - loss: 31.1346 - att_decoder_loss: 37.7481 - ctc_decoder_loss: 11.2943 - att_decoder_acc: 0.8017 - val_loss: 31.9020 - val_att_decoder_loss: 37.4719 - val_ctc_decoder_loss: 15.1921 - val_att_decoder_acc: 0.8226\n",
            "Epoch 22/78\n",
            "420/420 [==============================] - 144s 344ms/step - loss: 31.1368 - att_decoder_loss: 37.7600 - ctc_decoder_loss: 11.2673 - att_decoder_acc: 0.8021 - val_loss: 31.8494 - val_att_decoder_loss: 37.4302 - val_ctc_decoder_loss: 15.1071 - val_att_decoder_acc: 0.8231\n",
            "Epoch 23/78\n",
            "420/420 [==============================] - 145s 344ms/step - loss: 31.0890 - att_decoder_loss: 37.7112 - ctc_decoder_loss: 11.2222 - att_decoder_acc: 0.8028 - val_loss: 31.8412 - val_att_decoder_loss: 37.4352 - val_ctc_decoder_loss: 15.0594 - val_att_decoder_acc: 0.8230\n",
            "Epoch 24/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 31.0374 - att_decoder_loss: 37.6597 - ctc_decoder_loss: 11.1704 - att_decoder_acc: 0.8036 - val_loss: 31.8870 - val_att_decoder_loss: 37.4646 - val_ctc_decoder_loss: 15.1539 - val_att_decoder_acc: 0.8226\n",
            "Epoch 25/78\n",
            "420/420 [==============================] - 144s 342ms/step - loss: 31.0673 - att_decoder_loss: 37.6799 - ctc_decoder_loss: 11.2295 - att_decoder_acc: 0.8028 - val_loss: 31.8895 - val_att_decoder_loss: 37.4499 - val_ctc_decoder_loss: 15.2085 - val_att_decoder_acc: 0.8229\n",
            "Epoch 26/78\n",
            "420/420 [==============================] - 143s 341ms/step - loss: 31.0206 - att_decoder_loss: 37.6397 - ctc_decoder_loss: 11.1634 - att_decoder_acc: 0.8037 - val_loss: 31.8890 - val_att_decoder_loss: 37.4720 - val_ctc_decoder_loss: 15.1399 - val_att_decoder_acc: 0.8223\n",
            "Epoch 27/78\n",
            "420/420 [==============================] - 144s 344ms/step - loss: 31.0578 - att_decoder_loss: 37.6809 - ctc_decoder_loss: 11.1885 - att_decoder_acc: 0.8033 - val_loss: 31.8968 - val_att_decoder_loss: 37.4700 - val_ctc_decoder_loss: 15.1774 - val_att_decoder_acc: 0.8224\n",
            "Epoch 28/78\n",
            "420/420 [==============================] - 143s 341ms/step - loss: 31.0629 - att_decoder_loss: 37.6900 - ctc_decoder_loss: 11.1814 - att_decoder_acc: 0.8034 - val_loss: 31.8622 - val_att_decoder_loss: 37.4291 - val_ctc_decoder_loss: 15.1616 - val_att_decoder_acc: 0.8232\n",
            "Epoch 29/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 31.0436 - att_decoder_loss: 37.6648 - ctc_decoder_loss: 11.1800 - att_decoder_acc: 0.8030 - val_loss: 31.8634 - val_att_decoder_loss: 37.4384 - val_ctc_decoder_loss: 15.1384 - val_att_decoder_acc: 0.8227\n",
            "Epoch 30/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.9931 - att_decoder_loss: 37.6156 - ctc_decoder_loss: 11.1254 - att_decoder_acc: 0.8039 - val_loss: 31.8547 - val_att_decoder_loss: 37.4307 - val_ctc_decoder_loss: 15.1267 - val_att_decoder_acc: 0.8231\n",
            "Epoch 31/78\n",
            "420/420 [==============================] - 141s 337ms/step - loss: 30.9974 - att_decoder_loss: 37.6277 - ctc_decoder_loss: 11.1067 - att_decoder_acc: 0.8048 - val_loss: 31.8935 - val_att_decoder_loss: 37.4583 - val_ctc_decoder_loss: 15.1993 - val_att_decoder_acc: 0.8226\n",
            "Epoch 32/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 31.0065 - att_decoder_loss: 37.6308 - ctc_decoder_loss: 11.1334 - att_decoder_acc: 0.8047 - val_loss: 31.8602 - val_att_decoder_loss: 37.4276 - val_ctc_decoder_loss: 15.1580 - val_att_decoder_acc: 0.8231\n",
            "Epoch 33/78\n",
            "420/420 [==============================] - 144s 343ms/step - loss: 30.9938 - att_decoder_loss: 37.6226 - ctc_decoder_loss: 11.1073 - att_decoder_acc: 0.8043 - val_loss: 31.8324 - val_att_decoder_loss: 37.4106 - val_ctc_decoder_loss: 15.0977 - val_att_decoder_acc: 0.8235\n",
            "Epoch 34/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 31.0204 - att_decoder_loss: 37.6505 - ctc_decoder_loss: 11.1300 - att_decoder_acc: 0.8041 - val_loss: 31.8742 - val_att_decoder_loss: 37.4413 - val_ctc_decoder_loss: 15.1729 - val_att_decoder_acc: 0.8231\n",
            "Epoch 35/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.9637 - att_decoder_loss: 37.5955 - ctc_decoder_loss: 11.0684 - att_decoder_acc: 0.8053 - val_loss: 31.8736 - val_att_decoder_loss: 37.4576 - val_ctc_decoder_loss: 15.1214 - val_att_decoder_acc: 0.8228\n",
            "Epoch 36/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.9916 - att_decoder_loss: 37.6149 - ctc_decoder_loss: 11.1216 - att_decoder_acc: 0.8042 - val_loss: 31.8630 - val_att_decoder_loss: 37.4312 - val_ctc_decoder_loss: 15.1582 - val_att_decoder_acc: 0.8233\n",
            "Epoch 37/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.9499 - att_decoder_loss: 37.5838 - ctc_decoder_loss: 11.0483 - att_decoder_acc: 0.8052 - val_loss: 31.8734 - val_att_decoder_loss: 37.4619 - val_ctc_decoder_loss: 15.1080 - val_att_decoder_acc: 0.8223\n",
            "Epoch 38/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 30.9929 - att_decoder_loss: 37.6231 - ctc_decoder_loss: 11.1022 - att_decoder_acc: 0.8050 - val_loss: 31.8985 - val_att_decoder_loss: 37.4680 - val_ctc_decoder_loss: 15.1900 - val_att_decoder_acc: 0.8225\n",
            "Epoch 39/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 30.9497 - att_decoder_loss: 37.5881 - ctc_decoder_loss: 11.0344 - att_decoder_acc: 0.8053 - val_loss: 31.8348 - val_att_decoder_loss: 37.4278 - val_ctc_decoder_loss: 15.0561 - val_att_decoder_acc: 0.8231\n",
            "Epoch 40/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.9403 - att_decoder_loss: 37.5740 - ctc_decoder_loss: 11.0392 - att_decoder_acc: 0.8061 - val_loss: 31.8844 - val_att_decoder_loss: 37.4667 - val_ctc_decoder_loss: 15.1374 - val_att_decoder_acc: 0.8227\n",
            "Epoch 41/78\n",
            "420/420 [==============================] - 143s 339ms/step - loss: 31.0083 - att_decoder_loss: 37.6370 - ctc_decoder_loss: 11.1224 - att_decoder_acc: 0.8040 - val_loss: 31.8528 - val_att_decoder_loss: 37.4349 - val_ctc_decoder_loss: 15.1063 - val_att_decoder_acc: 0.8230\n",
            "Epoch 42/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.9108 - att_decoder_loss: 37.5529 - ctc_decoder_loss: 10.9848 - att_decoder_acc: 0.8064 - val_loss: 31.8591 - val_att_decoder_loss: 37.4411 - val_ctc_decoder_loss: 15.1132 - val_att_decoder_acc: 0.8229\n",
            "Epoch 43/78\n",
            "420/420 [==============================] - 144s 342ms/step - loss: 30.8953 - att_decoder_loss: 37.5316 - ctc_decoder_loss: 10.9865 - att_decoder_acc: 0.8064 - val_loss: 31.8677 - val_att_decoder_loss: 37.4347 - val_ctc_decoder_loss: 15.1665 - val_att_decoder_acc: 0.8230\n",
            "Epoch 44/78\n",
            "420/420 [==============================] - 143s 339ms/step - loss: 30.9215 - att_decoder_loss: 37.5548 - ctc_decoder_loss: 11.0218 - att_decoder_acc: 0.8058 - val_loss: 31.8512 - val_att_decoder_loss: 37.4404 - val_ctc_decoder_loss: 15.0837 - val_att_decoder_acc: 0.8231\n",
            "Epoch 45/78\n",
            "420/420 [==============================] - 144s 344ms/step - loss: 30.9238 - att_decoder_loss: 37.5562 - ctc_decoder_loss: 11.0267 - att_decoder_acc: 0.8058 - val_loss: 31.8161 - val_att_decoder_loss: 37.4047 - val_ctc_decoder_loss: 15.0501 - val_att_decoder_acc: 0.8235\n",
            "Epoch 46/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.8988 - att_decoder_loss: 37.5380 - ctc_decoder_loss: 10.9814 - att_decoder_acc: 0.8070 - val_loss: 31.8268 - val_att_decoder_loss: 37.4100 - val_ctc_decoder_loss: 15.0771 - val_att_decoder_acc: 0.8236\n",
            "Epoch 47/78\n",
            "420/420 [==============================] - 141s 337ms/step - loss: 30.9297 - att_decoder_loss: 37.5569 - ctc_decoder_loss: 11.0482 - att_decoder_acc: 0.8059 - val_loss: 31.8499 - val_att_decoder_loss: 37.4261 - val_ctc_decoder_loss: 15.1211 - val_att_decoder_acc: 0.8232\n",
            "Epoch 48/78\n",
            "420/420 [==============================] - 148s 353ms/step - loss: 30.8787 - att_decoder_loss: 37.5192 - ctc_decoder_loss: 10.9574 - att_decoder_acc: 0.8066 - val_loss: 31.8581 - val_att_decoder_loss: 37.4370 - val_ctc_decoder_loss: 15.1215 - val_att_decoder_acc: 0.8231\n",
            "Epoch 49/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.9233 - att_decoder_loss: 37.5684 - ctc_decoder_loss: 10.9882 - att_decoder_acc: 0.8060 - val_loss: 31.8578 - val_att_decoder_loss: 37.4319 - val_ctc_decoder_loss: 15.1356 - val_att_decoder_acc: 0.8233\n",
            "Epoch 50/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.9106 - att_decoder_loss: 37.5431 - ctc_decoder_loss: 11.0130 - att_decoder_acc: 0.8059 - val_loss: 31.8647 - val_att_decoder_loss: 37.4337 - val_ctc_decoder_loss: 15.1576 - val_att_decoder_acc: 0.8232\n",
            "Epoch 51/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8764 - att_decoder_loss: 37.5232 - ctc_decoder_loss: 10.9359 - att_decoder_acc: 0.8075 - val_loss: 31.8588 - val_att_decoder_loss: 37.4321 - val_ctc_decoder_loss: 15.1390 - val_att_decoder_acc: 0.8232\n",
            "Epoch 52/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 30.8772 - att_decoder_loss: 37.5163 - ctc_decoder_loss: 10.9599 - att_decoder_acc: 0.8066 - val_loss: 31.8527 - val_att_decoder_loss: 37.4345 - val_ctc_decoder_loss: 15.1076 - val_att_decoder_acc: 0.8232\n",
            "Epoch 53/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 30.8493 - att_decoder_loss: 37.4806 - ctc_decoder_loss: 10.9554 - att_decoder_acc: 0.8073 - val_loss: 31.8468 - val_att_decoder_loss: 37.4178 - val_ctc_decoder_loss: 15.1338 - val_att_decoder_acc: 0.8232\n",
            "Epoch 54/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8558 - att_decoder_loss: 37.4996 - ctc_decoder_loss: 10.9242 - att_decoder_acc: 0.8077 - val_loss: 31.8386 - val_att_decoder_loss: 37.4152 - val_ctc_decoder_loss: 15.1086 - val_att_decoder_acc: 0.8235\n",
            "Epoch 55/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8531 - att_decoder_loss: 37.4927 - ctc_decoder_loss: 10.9343 - att_decoder_acc: 0.8074 - val_loss: 31.8485 - val_att_decoder_loss: 37.4294 - val_ctc_decoder_loss: 15.1056 - val_att_decoder_acc: 0.8230\n",
            "Epoch 56/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8736 - att_decoder_loss: 37.5143 - ctc_decoder_loss: 10.9513 - att_decoder_acc: 0.8071 - val_loss: 31.8614 - val_att_decoder_loss: 37.4260 - val_ctc_decoder_loss: 15.1676 - val_att_decoder_acc: 0.8234\n",
            "Epoch 57/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.8528 - att_decoder_loss: 37.4959 - ctc_decoder_loss: 10.9236 - att_decoder_acc: 0.8072 - val_loss: 31.8487 - val_att_decoder_loss: 37.4229 - val_ctc_decoder_loss: 15.1262 - val_att_decoder_acc: 0.8235\n",
            "Epoch 58/78\n",
            "420/420 [==============================] - 143s 342ms/step - loss: 30.8230 - att_decoder_loss: 37.4614 - ctc_decoder_loss: 10.9079 - att_decoder_acc: 0.8080 - val_loss: 31.8361 - val_att_decoder_loss: 37.4100 - val_ctc_decoder_loss: 15.1143 - val_att_decoder_acc: 0.8239\n",
            "Epoch 59/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8744 - att_decoder_loss: 37.5148 - ctc_decoder_loss: 10.9532 - att_decoder_acc: 0.8074 - val_loss: 31.8620 - val_att_decoder_loss: 37.4464 - val_ctc_decoder_loss: 15.1091 - val_att_decoder_acc: 0.8228\n",
            "Epoch 60/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8015 - att_decoder_loss: 37.4489 - ctc_decoder_loss: 10.8593 - att_decoder_acc: 0.8080 - val_loss: 31.8570 - val_att_decoder_loss: 37.4234 - val_ctc_decoder_loss: 15.1577 - val_att_decoder_acc: 0.8232\n",
            "Epoch 61/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8553 - att_decoder_loss: 37.4944 - ctc_decoder_loss: 10.9382 - att_decoder_acc: 0.8074 - val_loss: 31.8395 - val_att_decoder_loss: 37.4216 - val_ctc_decoder_loss: 15.0932 - val_att_decoder_acc: 0.8238\n",
            "Epoch 62/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8315 - att_decoder_loss: 37.4788 - ctc_decoder_loss: 10.8897 - att_decoder_acc: 0.8075 - val_loss: 31.8534 - val_att_decoder_loss: 37.4198 - val_ctc_decoder_loss: 15.1540 - val_att_decoder_acc: 0.8234\n",
            "Epoch 63/78\n",
            "420/420 [==============================] - 143s 340ms/step - loss: 30.8575 - att_decoder_loss: 37.4955 - ctc_decoder_loss: 10.9437 - att_decoder_acc: 0.8073 - val_loss: 31.8464 - val_att_decoder_loss: 37.4230 - val_ctc_decoder_loss: 15.1164 - val_att_decoder_acc: 0.8232\n",
            "Epoch 64/78\n",
            "420/420 [==============================] - 143s 341ms/step - loss: 30.8573 - att_decoder_loss: 37.5052 - ctc_decoder_loss: 10.9139 - att_decoder_acc: 0.8080 - val_loss: 31.8517 - val_att_decoder_loss: 37.4291 - val_ctc_decoder_loss: 15.1196 - val_att_decoder_acc: 0.8233\n",
            "Epoch 65/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8423 - att_decoder_loss: 37.4771 - ctc_decoder_loss: 10.9381 - att_decoder_acc: 0.8071 - val_loss: 31.8410 - val_att_decoder_loss: 37.4197 - val_ctc_decoder_loss: 15.1052 - val_att_decoder_acc: 0.8235\n",
            "Epoch 66/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8508 - att_decoder_loss: 37.4912 - ctc_decoder_loss: 10.9296 - att_decoder_acc: 0.8072 - val_loss: 31.8464 - val_att_decoder_loss: 37.4196 - val_ctc_decoder_loss: 15.1265 - val_att_decoder_acc: 0.8236\n",
            "Epoch 67/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8377 - att_decoder_loss: 37.4850 - ctc_decoder_loss: 10.8958 - att_decoder_acc: 0.8081 - val_loss: 31.8226 - val_att_decoder_loss: 37.3983 - val_ctc_decoder_loss: 15.0956 - val_att_decoder_acc: 0.8236\n",
            "Epoch 68/78\n",
            "420/420 [==============================] - 142s 339ms/step - loss: 30.8025 - att_decoder_loss: 37.4501 - ctc_decoder_loss: 10.8596 - att_decoder_acc: 0.8086 - val_loss: 31.8176 - val_att_decoder_loss: 37.3928 - val_ctc_decoder_loss: 15.0920 - val_att_decoder_acc: 0.8240\n",
            "Epoch 69/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 30.8071 - att_decoder_loss: 37.4577 - ctc_decoder_loss: 10.8554 - att_decoder_acc: 0.8083 - val_loss: 31.8411 - val_att_decoder_loss: 37.4178 - val_ctc_decoder_loss: 15.1109 - val_att_decoder_acc: 0.8233\n",
            "Epoch 70/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8511 - att_decoder_loss: 37.4917 - ctc_decoder_loss: 10.9292 - att_decoder_acc: 0.8078 - val_loss: 31.8276 - val_att_decoder_loss: 37.4023 - val_ctc_decoder_loss: 15.1033 - val_att_decoder_acc: 0.8237\n",
            "Epoch 71/78\n",
            "420/420 [==============================] - 142s 337ms/step - loss: 30.8440 - att_decoder_loss: 37.4892 - ctc_decoder_loss: 10.9084 - att_decoder_acc: 0.8074 - val_loss: 31.8496 - val_att_decoder_loss: 37.4223 - val_ctc_decoder_loss: 15.1318 - val_att_decoder_acc: 0.8233\n",
            "Epoch 72/78\n",
            "420/420 [==============================] - 142s 338ms/step - loss: 30.8096 - att_decoder_loss: 37.4525 - ctc_decoder_loss: 10.8809 - att_decoder_acc: 0.8080 - val_loss: 31.8424 - val_att_decoder_loss: 37.4239 - val_ctc_decoder_loss: 15.0980 - val_att_decoder_acc: 0.8234\n",
            "Epoch 73/78\n",
            "420/420 [==============================] - 141s 337ms/step - loss: 30.8166 - att_decoder_loss: 37.4637 - ctc_decoder_loss: 10.8754 - att_decoder_acc: 0.8082 - val_loss: 31.8543 - val_att_decoder_loss: 37.4254 - val_ctc_decoder_loss: 15.1410 - val_att_decoder_acc: 0.8233\n",
            "Epoch 74/78\n",
            "420/420 [==============================] - 141s 336ms/step - loss: 30.8088 - att_decoder_loss: 37.4569 - ctc_decoder_loss: 10.8646 - att_decoder_acc: 0.8082 - val_loss: 31.8487 - val_att_decoder_loss: 37.4231 - val_ctc_decoder_loss: 15.1257 - val_att_decoder_acc: 0.8233\n",
            "Epoch 75/78\n",
            "420/420 [==============================] - 141s 337ms/step - loss: 30.8237 - att_decoder_loss: 37.4663 - ctc_decoder_loss: 10.8962 - att_decoder_acc: 0.8081 - val_loss: 31.8447 - val_att_decoder_loss: 37.4190 - val_ctc_decoder_loss: 15.1219 - val_att_decoder_acc: 0.8233\n",
            "Epoch 76/78\n",
            "420/420 [==============================] - 141s 336ms/step - loss: 30.8390 - att_decoder_loss: 37.4787 - ctc_decoder_loss: 10.9198 - att_decoder_acc: 0.8076 - val_loss: 31.8414 - val_att_decoder_loss: 37.4237 - val_ctc_decoder_loss: 15.0945 - val_att_decoder_acc: 0.8233\n",
            "Epoch 77/78\n",
            "420/420 [==============================] - 141s 336ms/step - loss: 30.8813 - att_decoder_loss: 37.5248 - ctc_decoder_loss: 10.9511 - att_decoder_acc: 0.8065 - val_loss: 31.8423 - val_att_decoder_loss: 37.4150 - val_ctc_decoder_loss: 15.1242 - val_att_decoder_acc: 0.8235\n",
            "Epoch 78/78\n",
            "420/420 [==============================] - 141s 336ms/step - loss: 30.8177 - att_decoder_loss: 37.4618 - ctc_decoder_loss: 10.8855 - att_decoder_acc: 0.8081 - val_loss: 31.8572 - val_att_decoder_loss: 37.4258 - val_ctc_decoder_loss: 15.1514 - val_att_decoder_acc: 0.8233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function Executor.__del__ at 0x7b187f85b7f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
            "    self.wait()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATTENTION EVAL\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c1f4380c8274391a8e5126b9d06bc3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function Executor.__del__ at 0x7b187f85b7f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
            "    self.wait()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target    : 289-319-6224\n",
            "Prediction: 289-319-6224\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : 8804 belfry drive\n",
            "Prediction: 8804 bea fradill\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : pirma.cam.gov.co/life-here\n",
            "Prediction: pirma.cam.gov.co/life-here\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : 35816 webwood\n",
            "Prediction: 35816 webwood\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : arabiiinet.com/241,112,183/\n",
            "Prediction: arabainet.com/241/112/183/\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Score: 0.8019\n",
            "mean_dist: 3.5570\n",
            "\n",
            "CTC EVAL\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65428bcbcbec4928aa1631b1badd2a9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function Executor.__del__ at 0x7b187f85b7f0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
            "    self.wait()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target    : 109 sparks station\n",
            "Prediction: 109 sparks station\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : www.prxclinical.com/\n",
            "Prediction: www.prclinical.com/\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : 886-146-3586\n",
            "Prediction: 886-146-3586\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : josefina mata\n",
            "Prediction: josefina mata\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Target    : kimiafarmacare/table-rock\n",
            "Prediction: kimiafarmacare/table-rocck\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Score: 0.7970\n",
            "mean_dist: 3.6445\n"
          ]
        }
      ],
      "source": [
        "CFG.resume = 'auto'\n",
        "# CFG.resume = 134\n",
        "# CFG.resume_ckpt = f'{CFG.comment}-fold0-best.h5'\n",
        "train_folds(CFG, [0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WTJ8eruttPxJ"
      },
      "outputs": [],
      "source": [
        "# # CFG.resume = 'auto'\n",
        "# CFG.seed = 42\n",
        "# CFG.comment = f'aslfr-fp16-192d-17l-ctcattjoint-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "y8IvRT7btP2E"
      },
      "outputs": [],
      "source": [
        "# CFG.seed = 43\n",
        "# CFG.comment = f'aslfr-fp16-192d-17l-ctcattjoint-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rHLmUWM4tP9S"
      },
      "outputs": [],
      "source": [
        "# CFG.seed = 44\n",
        "# CFG.comment = f'aslfr-fp16-192d-17l-ctcattjoint-seed{CFG.seed}'\n",
        "# train_folds(CFG, ['all'], summary=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4u3NIrDbhnk7"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}